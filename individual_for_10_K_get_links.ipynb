{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5015f591",
   "metadata": {},
   "source": [
    "# Setting Everything Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3283eb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0388ff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the file from the web\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from unicodedata import normalize\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pandas import read_csv \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import json\n",
    "from flatten_json import flatten\n",
    "\n",
    "\n",
    "# I can give a number or use None to remove maximum ceiling & display all columns\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# # I want to be able to see the entire narrative, so remove the maximum width for each column\n",
    "# pd.options.display.max_colwidth = None\n",
    "\n",
    "# pd.options.display.float_format = '{:,.0f}'.format\n",
    "\n",
    "import string\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42b42014",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline  \n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = (8, 8)\n",
    "\n",
    "import warnings\n",
    "from rpy2.rinterface import RRuntimeWarning\n",
    "warnings.filterwarnings(\"ignore\") # Ignore all warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=RRuntimeWarning) # Show some warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "242498c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "// Disable auto-scrolling\n",
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "// Disable auto-scrolling\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "739fccf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Loading required package: tidyverse\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â”€â”€ Attaching core tidyverse packages â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse 2.0.0 â”€â”€\n",
      "âœ” dplyr     1.1.2     âœ” readr     2.1.4\n",
      "âœ” forcats   1.0.0     âœ” stringr   1.5.0\n",
      "âœ” ggplot2   3.4.2     âœ” tibble    3.2.1\n",
      "âœ” lubridate 1.9.2     âœ” tidyr     1.3.0\n",
      "âœ” purrr     1.0.1     \n",
      "â”€â”€ Conflicts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ tidyverse_conflicts() â”€â”€\n",
      "âœ– dplyr::filter() masks stats::filter()\n",
      "âœ– dplyr::lag()    masks stats::lag()\n",
      "â„¹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "# My commonly used R imports\n",
    "\n",
    "require('tidyverse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4aa539ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To Download PDFs\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e57e778",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To import camelot and PDF-related items\n",
    "import camelot\n",
    "import ghostscript\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "079713d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Loading required package: RColorBrewer\n",
      "\n",
      "R[write to console]: Loading required package: NLP\n",
      "\n",
      "R[write to console]: \n",
      "Attaching package: â€˜NLPâ€™\n",
      "\n",
      "\n",
      "R[write to console]: The following object is masked from â€˜package:ggplot2â€™:\n",
      "\n",
      "    annotate\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "library(jpeg)\n",
    "library(wordcloud)\n",
    "library(RColorBrewer)\n",
    "library(wordcloud2)\n",
    "library(tm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d016e49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6da3362c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: \n",
      "Attaching package: â€˜plotlyâ€™\n",
      "\n",
      "\n",
      "R[write to console]: The following object is masked from â€˜package:ggplot2â€™:\n",
      "\n",
      "    last_plot\n",
      "\n",
      "\n",
      "R[write to console]: The following object is masked from â€˜package:statsâ€™:\n",
      "\n",
      "    filter\n",
      "\n",
      "\n",
      "R[write to console]: The following object is masked from â€˜package:graphicsâ€™:\n",
      "\n",
      "    layout\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "library(vistime)\n",
    "library(shiny)\n",
    "library(plotly)\n",
    "#for timeline plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6204946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecbb534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for playwright stuff\n",
    "import time\n",
    "from playwright.async_api import async_playwright\n",
    "import asyncio\n",
    "import nest_asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f182a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install flask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9204bc01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1675ca1f",
   "metadata": {},
   "source": [
    " # Getting 10K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a29138e",
   "metadata": {},
   "source": [
    "# Putting together code for finding the 10k link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55b0392",
   "metadata": {},
   "source": [
    "#### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "58943819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # whatever title is input\n",
    "# one_title_searchable=\"Apple Inc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "94ab5a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is final\n",
    "\n",
    "\n",
    "async def extracting_stock_compensation_for_one_company():\n",
    "    \n",
    "\n",
    "    async def getting_10_k_link(one_title_searchable):\n",
    "        #### ON THE SEC HOME PAGE\n",
    "        # going to sec page\n",
    "        sec_home_playwright = await async_playwright().start()\n",
    "        sec_home_browser = await sec_home_playwright.chromium.launch(headless = False)\n",
    "        sec_home_page = await sec_home_browser.new_page()\n",
    "\n",
    "        await sec_home_page.goto(\"https://www.sec.gov/edgar/searchedgar/companysearch\")\n",
    "        time.sleep(6)\n",
    "\n",
    "        try:\n",
    "\n",
    "            # input company name (here is where we can enter title variable from df from csv)\n",
    "\n",
    "            await sec_home_page.fill(\"xpath=/html/body/div[1]/div/div/div/div[3]/div/div[2]/div[2]/div/div/article/div/div/div[1]/div[2]/div/div/div/div[1]/form/div[2]/input\", one_title_searchable)\n",
    "            time.sleep(6)\n",
    "\n",
    "            # click search\n",
    "\n",
    "            await sec_home_page.click(\"xpath=/html/body/div[1]/div/div/div/div[3]/div/div[2]/div[2]/div/div/article/div/div/div[1]/div[2]/div/div/div/div[1]/form/div[2]/button\")\n",
    "            time.sleep(6)\n",
    "\n",
    "            #============\n",
    "            # ON THE NEW PAGE RESULTING WITH ALL COMPANY DOCS\n",
    "            #input 10K for search\n",
    "            await sec_home_page.fill(\"xpath=/html/body/div[4]/div[2]/form/table/tbody/tr[3]/td[1]/input\", \"10-K\")\n",
    "            time.sleep(6)\n",
    "\n",
    "            # click search\n",
    "\n",
    "            await sec_home_page.click(\"xpath=/html/body/div[4]/div[2]/form/table/tbody/tr[3]/td[5]/input[1]\")\n",
    "            time.sleep(6)\n",
    "\n",
    "\n",
    "            #get all 10K entries\n",
    "            list_of_all_ten_k_entries_response= await sec_home_page.content()\n",
    "\n",
    "            list_of_all_ten_k_entries_doc = BeautifulSoup(list_of_all_ten_k_entries_response, 'html.parser')\n",
    "\n",
    "            #get the first 10K\n",
    "            list_of_all_ten_k_entries_table = list_of_all_ten_k_entries_doc.find('table', class_='tableFile2')\n",
    "\n",
    "            ten_k_entries_rows = list_of_all_ten_k_entries_table.find_all('tr')  # get the first row because it has the most recent 10K\n",
    "\n",
    "            most_recent_ten_k_link = None\n",
    "\n",
    "            for one_ten_k_entries_row in ten_k_entries_rows:\n",
    "                one_ten_k_entries_row_cells = one_ten_k_entries_row.find_all('td')  # Assuming each cell is represented by a <td> tag\n",
    "            #     print(one_ten_k_entries_row_cells)\n",
    "\n",
    "                for one_ten_k_entries_row_cell in one_ten_k_entries_row_cells:\n",
    "            #         print(one_ten_k_entries_row_cell)\n",
    "                    if one_ten_k_entries_row_cell.text.strip() == '10-K':\n",
    "                        ten_k_document_link_href = one_ten_k_entries_row_cell.find_next('td').find('a')['href']\n",
    "\n",
    "                        ten_k_document_link_base = \"https://www.sec.gov/\"\n",
    "                        ten_k_document_link_tail = ten_k_document_link_href\n",
    "                        ten_k_document_link = ten_k_document_link_base + ten_k_document_link_tail\n",
    "\n",
    "            #             print(document_link)\n",
    "\n",
    "                        # Store the first 10-K link and break out of the loop\n",
    "                        most_recent_ten_k_link = ten_k_document_link\n",
    "                        break\n",
    "\n",
    "                if most_recent_ten_k_link is not None:\n",
    "                    break\n",
    "\n",
    "\n",
    "            # ONCE I HAVE THE TEN K DOC LINK \n",
    "            # 3. Now on the page with the document and graphics table\n",
    "\n",
    "            ten_k_doc_link_playwright = await async_playwright().start()\n",
    "            ten_k_doc_link_browser = await ten_k_doc_link_playwright.chromium.launch(headless = False)\n",
    "            ten_k_doc_link_page = await ten_k_doc_link_browser.new_page()\n",
    "\n",
    "            time.sleep(6)\n",
    "\n",
    "            await ten_k_doc_link_page.goto(most_recent_ten_k_link)\n",
    "            time.sleep(6)\n",
    "\n",
    "\n",
    "\n",
    "            # Initiate the html and beautiful soup content \n",
    "\n",
    "            list_of_doc_and_graphics_response= await ten_k_doc_link_page.content()\n",
    "\n",
    "            list_of_doc_and_graphics_soup_doc = BeautifulSoup(list_of_doc_and_graphics_response, 'html.parser')\n",
    "\n",
    "            # get actual_full_ten_k_document_link\n",
    "            list_of_doc_and_graphics_table = list_of_doc_and_graphics_soup_doc.find('table', class_='tableFile')\n",
    "\n",
    "\n",
    "            list_of_doc_and_graphics_rows = list_of_doc_and_graphics_table.find_all('tr')  # Assuming each row is represented by a <tr> tag\n",
    "\n",
    "            for list_of_doc_and_graphics_row in list_of_doc_and_graphics_rows:\n",
    "                list_of_doc_and_graphics_cells = list_of_doc_and_graphics_row.find_all('td')  # Assuming each cell is represented by a <td> tag\n",
    "            #     print(cells)\n",
    "                for list_of_doc_and_graphics_cell in list_of_doc_and_graphics_cells:\n",
    "                    if list_of_doc_and_graphics_cell.text.strip() == '10-K':\n",
    "                        actual_document_link = list_of_doc_and_graphics_cell.find_next('td').find('a')\n",
    "                        if actual_document_link is not None:\n",
    "                            actual_ten_k_document_href = actual_document_link['href']\n",
    "            #                 print(\"Documents href:\", document_href)\n",
    "                            actual_ten_k_document_link_base=\"https://www.sec.gov/\"\n",
    "                            actual_ten_k_document_link_tail=actual_ten_k_document_href\n",
    "                            actual_full_ten_k_document_link=actual_ten_k_document_link_base+actual_ten_k_document_link_tail\n",
    "#                             global x\n",
    "                            x=actual_full_ten_k_document_link\n",
    "                            print(actual_full_ten_k_document_link)\n",
    "                            print(\"#########\")\n",
    "\n",
    "                            \n",
    "\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        finally:\n",
    "            # Close the page and browser\n",
    "            await ten_k_doc_link_page.close()\n",
    "            await ten_k_doc_link_browser.close()\n",
    "            await ten_k_doc_link_playwright.stop()\n",
    "\n",
    "            # Close the sec_home page and browser\n",
    "            await sec_home_page.close()\n",
    "            await sec_home_browser.close()\n",
    "            await sec_home_playwright.stop()\n",
    "            return actual_full_ten_k_document_link\n",
    "    \n",
    "    \n",
    "    \n",
    "    x = await getting_10_k_link(one_title_searchable)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # with change for link\n",
    "\n",
    "\n",
    "    list_of_actual_full_ten_k_document_links = [\"https://www.sec.gov/ix?doc=/Archives/edgar/data/789019/000156459022026876/msft-10k_20220630.htm\"]\n",
    "\n",
    "    actual_list_of_actual_full_ten_k_document_links = [\"https://www.sec.gov//ix?doc=/Archives/edgar/data/320193/000032019322000108/aapl-20220924.htm\",\"https://www.sec.gov/ix?doc=/Archives/edgar/data/789019/000156459022026876/msft-10k_20220630.htm\",\"https://www.sec.gov/ix?doc=/Archives/edgar/data/1652044/000165204423000016/goog-20221231.htm\"]\n",
    "\n",
    "    async def get_actual_full_ten_k_document_content(x):\n",
    "        playwright = await async_playwright().start()\n",
    "        browser = await playwright.chromium.launch(headless=False)\n",
    "    #     browser = await playwright.firefox.launch(headless=False)\n",
    "\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        await page.goto(x)\n",
    "        time.sleep(10)\n",
    "\n",
    "        while True:\n",
    "            # Get the number of spans on the page\n",
    "            spans_assigned = await page.query_selector_all(\"span\")\n",
    "            test = len(spans_assigned)\n",
    "\n",
    "            # Scroll in increments of 5000 pixels\n",
    "            await page.evaluate('''() => {\n",
    "                let distance = 5000;\n",
    "                window.scrollBy(0, distance);\n",
    "            }''')\n",
    "    #         await asyncio.sleep(3)\n",
    "\n",
    "            time.sleep(10)\n",
    "\n",
    "\n",
    "            spans_assigned_two = await page.query_selector_all(\"span\")\n",
    "            test2 = len(spans_assigned_two)\n",
    "            if test == test2:\n",
    "                break\n",
    "\n",
    "        # Get the content of the page\n",
    "        actual_full_ten_k_document_content = await page.content()\n",
    "        time.sleep(10)\n",
    "\n",
    "        await browser.close()\n",
    "        time.sleep(10)\n",
    "        await page.close()  # Add this line to close the page\n",
    "        time.sleep(10)\n",
    "\n",
    "        return actual_full_ten_k_document_content\n",
    "\n",
    "\n",
    "    async def process_actual_full_ten_k_document(actual_full_ten_k_document_content):\n",
    "        actual_full_ten_k_document_soup = BeautifulSoup(actual_full_ten_k_document_content, 'html.parser')\n",
    "        return actual_full_ten_k_document_soup\n",
    "\n",
    "\n",
    "    async def main():\n",
    "        dict_titles=['actual_full_ten_k_document_link_collection',\n",
    "                     'table_length',\n",
    "                     'list_of_stock_based_values',\n",
    "                     'first_stock_based_compensation',\n",
    "                     'second_stock_based_compensation',\n",
    "                     'third_stock_based_compensation']\n",
    "\n",
    "        possible_titles = ['CONSOLIDATED STATEMENTS OF CASH FLOWS',\n",
    "                           'CASH FLOWS STATEMENTS',\n",
    "                           'Consolidated Statements of Cash Flows',\n",
    "                           'Consolidated Statement of Cash Flows',\n",
    "                          'CONSOLIDATED STATEMENT OF CASH FLOWS',\n",
    "                          'STATEMENTS OF CONSOLIDATED CASH FLOWS',\n",
    "                          'Consolidated Cash Flow Statement',\n",
    "                          'Consolidated Statement of Cash Flow for the Years Ended December 31',\n",
    "                          'STATEMENT OF CASH FLOWS',\n",
    "                          'CONSOLIDATED STATEMENTS OF CASH FLOW',\n",
    "                          'Statements of Consolidated Cash Flows']\n",
    "\n",
    "        stock_related_texts = [\n",
    "            'Stock-based compensation expense',\n",
    "            'Stock-based compensation expense',\n",
    "            'Share-based compensation expense',\n",
    "            'Stock-based compensation',\n",
    "            'Share-based compensation'\n",
    "        ]\n",
    "\n",
    "        all_ten_k_document_lists = []\n",
    "\n",
    "\n",
    "    #     list_of_example_actual_full_ten_k_document_soup_doc = []\n",
    "\n",
    "        # Define the group size\n",
    "        group_size = 2\n",
    "        items=[x]\n",
    "\n",
    "        # Iterate over groups of items\n",
    "        for start_index in range(0, len(items), group_size):\n",
    "            group_items = items[start_index:start_index + group_size]\n",
    "\n",
    "            list_of_example_actual_full_ten_k_document_soup_doc = []\n",
    "\n",
    "\n",
    "    #         # Iterate over items within the group\n",
    "    #         for item in group_items:\n",
    "    #             print(item)\n",
    "\n",
    "    #         # Add a separator for each group\n",
    "    #         print(\"Group separator\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            url_to_parsed_content = {}\n",
    "\n",
    "\n",
    "            for actual_full_ten_k_document_link in group_items:\n",
    "                try:\n",
    "                    actual_full_ten_k_document_link_collection = actual_full_ten_k_document_link\n",
    "                    print(actual_full_ten_k_document_link)\n",
    "\n",
    "                    for _ in range(3):\n",
    "                        try:\n",
    "                            actual_full_ten_k_document_content = await get_actual_full_ten_k_document_content(actual_full_ten_k_document_link)\n",
    "                            example_actual_full_ten_k_document_soup_doc = await process_actual_full_ten_k_document(actual_full_ten_k_document_content)\n",
    "                            list_of_example_actual_full_ten_k_document_soup_doc.append(example_actual_full_ten_k_document_soup_doc)\n",
    "                                # Store the parsed content in the dictionary.\n",
    "                            url_to_parsed_content[actual_full_ten_k_document_link] = example_actual_full_ten_k_document_soup_doc\n",
    "\n",
    "                            break  # Break the loop if successful\n",
    "                        except Exception as e:\n",
    "                            print(f\"An error occurred: {str(e)}. Retrying...\")\n",
    "                            time.sleep(15)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            for actual_full_ten_k_document_link, one_actual_full_ten_k_document_soup_doc in url_to_parsed_content.items():\n",
    "                ten_k_detail_list=[]\n",
    "\n",
    "                if one_actual_full_ten_k_document_soup_doc:\n",
    "\n",
    "                    elements = one_actual_full_ten_k_document_soup_doc.find_all(text=possible_titles)\n",
    "\n",
    "                    if not elements:\n",
    "                        print(\"Titles not found in the HTML.\")\n",
    "                        continue\n",
    "\n",
    "                    for element in elements:\n",
    "                        if element.strip() in possible_titles:\n",
    "                            following_elements = element.find_all_next()\n",
    "                            table = None\n",
    "\n",
    "                            # Iterate through following elements and stop when a table is found or after three components\n",
    "                            component_count = 0\n",
    "                            for next_element in following_elements:\n",
    "                                if next_element.name == 'table' and any(text in next_element.get_text() for text in stock_related_texts):\n",
    "                                    table = next_element\n",
    "                                    break\n",
    "                                elif next_element.name in ['p', 'span']:\n",
    "                                    component_count += 1\n",
    "                                    if component_count >= 5:\n",
    "                                        break\n",
    "\n",
    "                            if table is None:\n",
    "                                tables = one_actual_full_ten_k_document_soup_doc.find_all(\"table\")\n",
    "                                for t in tables:     \n",
    "                                    if t:\n",
    "                                        table_text = t.get_text()\n",
    "                                        if any(title in table_text for title in possible_titles) and any(stock_related_text in table_text for stock_related_text in stock_related_texts):\n",
    "                                            rows = t.find_all('tr')\n",
    "                                            table_length = len(rows)\n",
    "                                            print(f\"Table Length from within: {len(rows)}\") \n",
    "                                        break\n",
    "\n",
    "                            if table:\n",
    "                                rows = table.find_all('tr')\n",
    "                                table_length=len(rows)\n",
    "\n",
    "                                print(f\"Table Length: {len(rows)}\")                        \n",
    "\n",
    "                                break\n",
    "\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        print(\"No table found within or after the possible titles in the HTML.\")\n",
    "\n",
    "                else:\n",
    "                    print(\"Document not processed, skipping\")\n",
    "\n",
    "\n",
    "                if table:\n",
    "                    target_td = table.find('td', text=stock_related_texts)\n",
    "                else:\n",
    "                    target_td = None\n",
    "\n",
    "                if target_td:\n",
    "                    # Move up to the <tr> containing the <td> with \"share-based compensation\"\n",
    "                    target_tr = target_td.find_parent('tr')\n",
    "\n",
    "                    # Extract all the <td> values from the <tr> and put them in a column\n",
    "                    values_in_column = [td.get_text(strip=True) for td in target_tr.find_all('td')]\n",
    "\n",
    "                    # Output the values in the column\n",
    "                    if values_in_column:\n",
    "                        stock_stock_based_compensation_title=values_in_column[0]\n",
    "                        list_of_stock_based_values=values_in_column\n",
    "                        numbers_only_values_in_column = [entry for entry in values_in_column if any(char.isdigit() for char in entry)]\n",
    "                        first_stock_based_compensation=numbers_only_values_in_column[0]\n",
    "                        second_stock_based_compensation=numbers_only_values_in_column[1]\n",
    "                        third_stock_based_compensation=numbers_only_values_in_column[2]\n",
    "                        for value in values_in_column:\n",
    "                            print(value)\n",
    "\n",
    "                    ten_k_detail_list.append(actual_full_ten_k_document_link)\n",
    "                    ten_k_detail_list.append(table_length)\n",
    "                    ten_k_detail_list.append(list_of_stock_based_values)\n",
    "                    ten_k_detail_list.append(first_stock_based_compensation)\n",
    "                    ten_k_detail_list.append(second_stock_based_compensation)\n",
    "                    ten_k_detail_list.append(third_stock_based_compensation)\n",
    "\n",
    "                    first_rep = dict(zip(dict_titles, ten_k_detail_list))\n",
    "                    all_ten_k_document_lists.append(first_rep)\n",
    "\n",
    "\n",
    "\n",
    "                else:\n",
    "                    print(\"The 'Share-Based Compensation' row not found!\")\n",
    "                time.sleep(10)\n",
    "\n",
    "    #     print(all_ten_k_document_lists)\n",
    "        return all_ten_k_document_lists\n",
    "\n",
    "\n",
    "    # Run the main coroutine\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    # Use asyncio.run() to run the main coroutine\n",
    "    all_ten_k_document_lists_of_dicts=asyncio.run(main())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e04a6826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running `brew update --auto-update`...\n",
      "\u0007\u001b[34m==>\u001b[0m \u001b[1mHomebrew collects anonymous analytics.\u001b[0m\n",
      "\u001b[1mRead the analytics documentation (and how to opt-out) here:\n",
      "  \u001b[4mhttps://docs.brew.sh/Analytics\u001b[24m\u001b[0m\n",
      "No analytics have been recorded yet (nor will be during this `brew` run).\n",
      "\n",
      "\u001b[34m==>\u001b[0m \u001b[1mhomebrew/core is old and unneeded, untapping to save space...\u001b[0m\n",
      "Untapping homebrew/core...\n",
      "Untapped 3 commands and 6530 formulae (7,125 files, 672.5MB).\n",
      "\u001b[34m==>\u001b[0m \u001b[1mhomebrew/cask is old and unneeded, untapping to save space...\u001b[0m\n",
      "Untapping homebrew/cask...\n",
      "Untapped 4161 casks (4,328 files, 362.6MB).\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://formulae.brew.sh/api/formula_tap_migrations.jws.json\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\u001b[34m==>\u001b[0m \u001b[1mAuto-updated Homebrew!\u001b[0m\n",
      "Updated 4 taps (homebrew/services, caskroom/cask, homebrew/core and homebrew/cask).\n",
      "\u001b[34m==>\u001b[0m \u001b[1mNew Formulae\u001b[0m\n",
      "adb-enhanced                             medusa\n",
      "aerleon                                  meson-python\n",
      "alass                                    mgis\n",
      "apify-cli                                minigraph\n",
      "arm-none-eabi-binutils                   mjml\n",
      "arm-none-eabi-gcc                        mvfst\n",
      "arm-none-eabi-gdb                        mvt\n",
      "asnmap                                   mysql-client@8.0\n",
      "bacon                                    mysql@8.0\n",
      "bazel-diff                               neonctl\n",
      "bazel-remote                             notation\n",
      "bbot                                     ntbtls\n",
      "bfs                                      ollama\n",
      "bilix                                    orbiton\n",
      "blades                                   pgrok\n",
      "blink                                    pixi\n",
      "botan@2                                  plog\n",
      "cargo-all-features                       pop\n",
      "cargo-auditable                          prettierd\n",
      "cargo-binstall                           proxify\n",
      "cargo-binutils                           proxygen\n",
      "cargo-deps                               pylyzer\n",
      "cargo-generate                           pypy3.10\n",
      "cbonsai                                  pypy3.9\n",
      "cdi                                      python-certifi\n",
      "cdxgen                                   python-click\n",
      "cloud-sql-proxy                          python-cryptography\n",
      "cloudlist                                python-flit-core\n",
      "codelimit                                python-lxml\n",
      "coder                                    python-markupsafe\n",
      "couchbase-shell                          python-packaging\n",
      "counts                                   python-pyparsing\n",
      "cppinsights                              python-pytz\n",
      "crabz                                    python-toml\n",
      "crystalline                              quictls\n",
      "ctpv                                     rio\n",
      "cycode                                   riscv64-elf-binutils\n",
      "czkawka                                  riscv64-elf-gcc\n",
      "dnsrobocert                              riscv64-elf-gdb\n",
      "dolphie                                  risor\n",
      "driftwood                                roadrunner\n",
      "dysk                                     rpmspectool\n",
      "ebook2cw                                 ruff-lsp\n",
      "erg                                      runme\n",
      "erlang@25                                s3scanner\n",
      "espflash                                 sbom-tool\n",
      "feishu2md                                scrapy\n",
      "fuc                                      sh4d0wup\n",
      "fw                                       shub\n",
      "getmail6                                 shuffledns\n",
      "go-feature-flag                          sickchill\n",
      "go@1.20                                  solr@8.11\n",
      "goread                                   sqlpage\n",
      "gotestsum                                strip-nondeterminism\n",
      "govulncheck                              terraform-graph-beautifier\n",
      "hex                                      terragrunt-atlantis-config\n",
      "hoverfly                                 toxiproxy\n",
      "hyfetch                                  tpm\n",
      "img2pdf                                  trufflehog\n",
      "imgdiet                                  trzsz-ssh\n",
      "imgdiff                                  typical\n",
      "ittapi                                   tzdiff\n",
      "katana                                   udp2raw-multiplatform\n",
      "killport                                 uncover\n",
      "kor                                      urlfinder\n",
      "ldid-procursus                           vunnel\n",
      "legitify                                 webpod\n",
      "libdivsufsort                            wget2\n",
      "libpanel                                 woof-doom\n",
      "libshumate                               wpscan\n",
      "libversion                               wtfis\n",
      "llm                                      xe\n",
      "lr                                       xlsxio\n",
      "lune                                     yazi\n",
      "lxi-tools                                yyjson\n",
      "mailpit                                  zchunk\n",
      "mariadb@11.0                             zrok\n",
      "massdriver\n",
      "\u001b[34m==>\u001b[0m \u001b[1mNew Casks\u001b[0m\n",
      "4k-video-downloaderplus    font-finagler              poe\n",
      "4k-video-downloaderplus    frappe-books               poe\n",
      "apidog                     frappe-books               replay\n",
      "apidog                     git-credential-manager     replay\n",
      "applite                    git-credential-manager     rio\n",
      "applite                    glaze                      rio\n",
      "audiocupcake               glaze                      ripx\n",
      "audiocupcake               graalvm-jdk                ripx\n",
      "bepo                       graalvm-jdk                score\n",
      "bepo                       herd                       score\n",
      "browser-deputy             herd                       screen-studio\n",
      "browser-deputy             hovrly                     screen-studio\n",
      "chatall                    hovrly                     sfm\n",
      "chatall                    hypercal                   sfm\n",
      "cilicon                    hypercal                   shattered-pixel-dungeon\n",
      "cilicon                    iem-plugin-suite           shattered-pixel-dungeon\n",
      "clop                       iem-plugin-suite           showmeyourhotkeys\n",
      "clop                       json-viewer                showmeyourhotkeys\n",
      "command-x                  json-viewer                simple-web-server\n",
      "command-x                  jukebox                    simple-web-server\n",
      "crystalfetch               jukebox                    sparkplate\n",
      "crystalfetch               keyclu                     sparkplate\n",
      "dockx                      keyclu                     stash\n",
      "dockx                      lm-studio                  stash\n",
      "drata-agent                lm-studio                  ths\n",
      "drata-agent                luniistore                 ths\n",
      "elektron-overbridge        luniistore                 updf\n",
      "elektron-overbridge        maa                        updf\n",
      "elektron-transfer          maa                        viso\n",
      "elektron-transfer          mycard                     viso\n",
      "energiza                   mycard                     voicepeak\n",
      "energiza                   picoscope                  voicepeak\n",
      "ente                       picoscope                  whisky\n",
      "ente                       pieces                     whisky\n",
      "fedistar                   pieces                     xiaomi-cloud\n",
      "fedistar                   pieces-os                  xiaomi-cloud\n",
      "flexoptix                  pieces-os                  zspace\n",
      "flexoptix                  piphero                    zspace\n",
      "font-finagler              piphero\n",
      "\n",
      "You have \u001b[1m76\u001b[0m outdated formulae and \u001b[1m3\u001b[0m outdated casks installed.\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://formulae.brew.sh/api/cask_tap_migrations.jws.json\u001b[0m\n",
      "######################################################################### 100.0%\n",
      "\n",
      "\u001b[34m==>\u001b[0m \u001b[1mTapping heroku/brew\u001b[0m\n",
      "Cloning into '/opt/homebrew/Library/Taps/heroku/homebrew-brew'...\n",
      "remote: Enumerating objects: 1915, done.\u001b[K\n",
      "remote: Counting objects: 100% (300/300), done.\u001b[K\n",
      "remote: Compressing objects: 100% (213/213), done.\u001b[K\n",
      "remote: Total 1915 (delta 94), reused 279 (delta 74), pack-reused 1615\u001b[K\n",
      "Receiving objects: 100% (1915/1915), 250.05 KiB | 6.10 MiB/s, done.\n",
      "Resolving deltas: 100% (490/490), done.\n",
      "Tapped 2 formulae (17 files, 311.2KB).\n",
      "\u001b[32m==>\u001b[0m \u001b[1mFetching \u001b[32mheroku/brew/heroku\u001b[39m\u001b[0m\n",
      "\u001b[34m==>\u001b[0m \u001b[1mDownloading https://cli-assets.heroku.com/versions/8.4.2/b451c6b/heroku-v8.4\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################### 100.0%\n",
      "\u001b[34m==>\u001b[0m \u001b[1mInstalling heroku from heroku/brew\u001b[0m\n",
      "\u001b[33mWarning:\u001b[0m A newer Command Line Tools release is available.\n",
      "Update them from Software Update in System Preferences.\n",
      "\n",
      "If that doesn't show you any updates, run:\n",
      "  sudo rm -rf /Library/Developer/CommandLineTools\n",
      "  sudo xcode-select --install\n",
      "\n",
      "Alternatively, manually download them from:\n",
      "  \u001b[4mhttps://developer.apple.com/download/all/\u001b[24m.\n",
      "You should download the Command Line Tools for Xcode 14.2.\n",
      "\n",
      "\u001b[34m==>\u001b[0m \u001b[1mCaveats\u001b[0m\n",
      "To use the Heroku CLI's autocomplete --\n",
      "  Via homebrew's shell completion:\n",
      "    1) Follow homebrew's install instructions https://docs.brew.sh/Shell-Completion\n",
      "        NOTE: For zsh, as the instructions mention, be sure compinit is autoloaded\n",
      "              and called, either explicitly or via a framework like oh-my-zsh.\n",
      "    2) Then run\n",
      "      $ heroku autocomplete --refresh-cache\n",
      "  OR\n",
      "  Use our standalone setup:\n",
      "    1) Run and follow the install steps:\n",
      "      $ heroku autocomplete\n",
      "\n",
      "zsh completions have been installed to:\n",
      "  /opt/homebrew/share/zsh/site-functions\n",
      "\u001b[34m==>\u001b[0m \u001b[1mSummary\u001b[0m\n",
      "ðŸº  /opt/homebrew/Cellar/heroku/8.4.2: 35,444 files, 177.6MB, built in 23 seconds\n",
      "\u001b[34m==>\u001b[0m \u001b[1mRunning `brew cleanup heroku`...\u001b[0m\n",
      "Disable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.\n",
      "Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n",
      "\u001b[32m==>\u001b[0m \u001b[1m`brew cleanup` has not been run in the last 30 days, running now...\u001b[0m\n",
      "Disable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.\n",
      "Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).\n",
      "Removing: /Users/ivynyayieka/Library/Caches/Homebrew/freetype--2.13.0_1... (911.3KB)\n",
      "Removing: /Users/ivynyayieka/Library/Caches/Homebrew/ghostscript--10.01.1_1... (52.7MB)\n",
      "Removing: /Users/ivynyayieka/Library/Caches/Homebrew/gnutls--3.8.0... (2.9MB)\n",
      "Removing: /Users/ivynyayieka/Library/Caches/Homebrew/highway--1.0.4... (477.9KB)\n",
      "Removing: /Users/ivynyayieka/Library/Caches/Homebrew/jpeg--9e... (308.4KB)\n",
      "Removing: /Users/ivynyayieka/Library/Caches/Homebrew/jpeg-turbo--2.1.5.1... (827KB)\n",
      "Removing: /Users/ivynyayieka/Library/Caches/Homebrew/leptonica--1.82.0_2... (2.5MB)\n",
      "Removing: /Users/ivynyayieka/Library/Caches/Homebrew/libarchive--3.6.2_1... (1.5MB)\n",
      "Removing: /Users/ivynyayieka/Library/Caches/Homebrew/libsndfile--1.2.0_1... (348.2KB)\n",
      "Removing: /Users/ivynyayieka/Library/Caches/Homebrew/libx11--1.8.4... (2.1MB)\n",
      "Removing: /Users/ivynyayieka/Library/Caches/Homebrew/mbedtls--3.4.0... (3MB)\n",
      "Removing: /Users/ivynyayieka/Library/Caches/Homebrew/pillow--9.5.0_1... (1.1MB)\n",
      "Removing: /Users/ivynyayieka/Library/Caches/Homebrew/pybind11--2.10.4... (564KB)\n",
      "Removing: /Users/ivynyayieka/Library/Caches/Homebrew/rubberband--3.2.1... (550.6KB)\n",
      "Removing: /Users/ivynyayieka/Library/Caches/Homebrew/sdl2--2.26.5... (1.7MB)\n",
      "Removing: /Users/ivynyayieka/Library/Caches/Homebrew/svt-av1--1.5.0... (1.4MB)\n",
      "Removing: /Users/ivynyayieka/Library/Caches/Homebrew/tesseract--5.3.1... (12.5MB)\n",
      "Removing: /Users/ivynyayieka/Library/Caches/Homebrew/unbound--1.17.1... (2.8MB)\n",
      "Removing: /Users/ivynyayieka/Library/Caches/Homebrew/webp--1.3.0_1... (791.6KB)\n",
      "Removing: /Users/ivynyayieka/Library/Caches/Homebrew/xz--5.4.3... (652.8KB)\n",
      "Removing: /Users/ivynyayieka/Library/Caches/Homebrew/jpeg_bottle_manifest--9e... (8KB)\n"
     ]
    }
   ],
   "source": [
    "!brew tap heroku/brew && brew install heroku\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56efcf43",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'ten_k_doc_link_page' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m extracting_stock_compensation_for_one_company()\n",
      "Cell \u001b[0;32mIn [17], line 139\u001b[0m, in \u001b[0;36mextracting_stock_compensation_for_one_company\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m sec_home_playwright\u001b[38;5;241m.\u001b[39mstop()\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m actual_full_ten_k_document_link\n\u001b[0;32m--> 139\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m getting_10_k_link(one_title_searchable)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# with change for link\u001b[39;00m\n\u001b[1;32m    149\u001b[0m list_of_actual_full_ten_k_document_links \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.sec.gov/ix?doc=/Archives/edgar/data/789019/000156459022026876/msft-10k_20220630.htm\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn [17], line 127\u001b[0m, in \u001b[0;36mextracting_stock_compensation_for_one_company.<locals>.getting_10_k_link\u001b[0;34m(one_title_searchable)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;66;03m# Close the page and browser\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43mten_k_doc_link_page\u001b[49m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m ten_k_doc_link_browser\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m ten_k_doc_link_playwright\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'ten_k_doc_link_page' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "await extracting_stock_compensation_for_one_company()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766939b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3026c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add26256",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98089e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whatever title is input\n",
    "one_title_searchable=\"Apple Inc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80816a5a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# this is final\n",
    "\n",
    "async def getting_10_k_link(one_title_searchable):\n",
    "    #### ON THE SEC HOME PAGE\n",
    "    # going to sec page\n",
    "    sec_home_playwright = await async_playwright().start()\n",
    "    sec_home_browser = await sec_home_playwright.chromium.launch(headless = False)\n",
    "    sec_home_page = await sec_home_browser.new_page()\n",
    "\n",
    "    await sec_home_page.goto(\"https://www.sec.gov/edgar/searchedgar/companysearch\")\n",
    "    time.sleep(6)\n",
    "\n",
    "    try:\n",
    "\n",
    "        # input company name (here is where we can enter title variable from df from csv)\n",
    "\n",
    "        await sec_home_page.fill(\"xpath=/html/body/div[1]/div/div/div/div[3]/div/div[2]/div[2]/div/div/article/div/div/div[1]/div[2]/div/div/div/div[1]/form/div[2]/input\", one_title_searchable)\n",
    "        time.sleep(6)\n",
    "\n",
    "        # click search\n",
    "\n",
    "        await sec_home_page.click(\"xpath=/html/body/div[1]/div/div/div/div[3]/div/div[2]/div[2]/div/div/article/div/div/div[1]/div[2]/div/div/div/div[1]/form/div[2]/button\")\n",
    "        time.sleep(6)\n",
    "\n",
    "        #============\n",
    "        # ON THE NEW PAGE RESULTING WITH ALL COMPANY DOCS\n",
    "        #input 10K for search\n",
    "        await sec_home_page.fill(\"xpath=/html/body/div[4]/div[2]/form/table/tbody/tr[3]/td[1]/input\", \"10-K\")\n",
    "        time.sleep(6)\n",
    "\n",
    "        # click search\n",
    "\n",
    "        await sec_home_page.click(\"xpath=/html/body/div[4]/div[2]/form/table/tbody/tr[3]/td[5]/input[1]\")\n",
    "        time.sleep(6)\n",
    "\n",
    "\n",
    "        #get all 10K entries\n",
    "        list_of_all_ten_k_entries_response= await sec_home_page.content()\n",
    "\n",
    "        list_of_all_ten_k_entries_doc = BeautifulSoup(list_of_all_ten_k_entries_response, 'html.parser')\n",
    "\n",
    "        #get the first 10K\n",
    "        list_of_all_ten_k_entries_table = list_of_all_ten_k_entries_doc.find('table', class_='tableFile2')\n",
    "\n",
    "        ten_k_entries_rows = list_of_all_ten_k_entries_table.find_all('tr')  # get the first row because it has the most recent 10K\n",
    "\n",
    "        most_recent_ten_k_link = None\n",
    "\n",
    "        for one_ten_k_entries_row in ten_k_entries_rows:\n",
    "            one_ten_k_entries_row_cells = one_ten_k_entries_row.find_all('td')  # Assuming each cell is represented by a <td> tag\n",
    "        #     print(one_ten_k_entries_row_cells)\n",
    "\n",
    "            for one_ten_k_entries_row_cell in one_ten_k_entries_row_cells:\n",
    "        #         print(one_ten_k_entries_row_cell)\n",
    "                if one_ten_k_entries_row_cell.text.strip() == '10-K':\n",
    "                    ten_k_document_link_href = one_ten_k_entries_row_cell.find_next('td').find('a')['href']\n",
    "\n",
    "                    ten_k_document_link_base = \"https://www.sec.gov/\"\n",
    "                    ten_k_document_link_tail = ten_k_document_link_href\n",
    "                    ten_k_document_link = ten_k_document_link_base + ten_k_document_link_tail\n",
    "\n",
    "        #             print(document_link)\n",
    "\n",
    "                    # Store the first 10-K link and break out of the loop\n",
    "                    most_recent_ten_k_link = ten_k_document_link\n",
    "                    break\n",
    "\n",
    "            if most_recent_ten_k_link is not None:\n",
    "                break\n",
    "\n",
    "\n",
    "        # ONCE I HAVE THE TEN K DOC LINK \n",
    "        # 3. Now on the page with the document and graphics table\n",
    "\n",
    "        ten_k_doc_link_playwright = await async_playwright().start()\n",
    "        ten_k_doc_link_browser = await ten_k_doc_link_playwright.chromium.launch(headless = False)\n",
    "        ten_k_doc_link_page = await ten_k_doc_link_browser.new_page()\n",
    "\n",
    "        time.sleep(6)\n",
    "\n",
    "        await ten_k_doc_link_page.goto(most_recent_ten_k_link)\n",
    "        time.sleep(6)\n",
    "\n",
    "\n",
    "\n",
    "        # Initiate the html and beautiful soup content \n",
    "\n",
    "        list_of_doc_and_graphics_response= await ten_k_doc_link_page.content()\n",
    "\n",
    "        list_of_doc_and_graphics_soup_doc = BeautifulSoup(list_of_doc_and_graphics_response, 'html.parser')\n",
    "\n",
    "        # get actual_full_ten_k_document_link\n",
    "        list_of_doc_and_graphics_table = list_of_doc_and_graphics_soup_doc.find('table', class_='tableFile')\n",
    "\n",
    "\n",
    "        list_of_doc_and_graphics_rows = list_of_doc_and_graphics_table.find_all('tr')  # Assuming each row is represented by a <tr> tag\n",
    "\n",
    "        for list_of_doc_and_graphics_row in list_of_doc_and_graphics_rows:\n",
    "            list_of_doc_and_graphics_cells = list_of_doc_and_graphics_row.find_all('td')  # Assuming each cell is represented by a <td> tag\n",
    "        #     print(cells)\n",
    "            for list_of_doc_and_graphics_cell in list_of_doc_and_graphics_cells:\n",
    "                if list_of_doc_and_graphics_cell.text.strip() == '10-K':\n",
    "                    actual_document_link = list_of_doc_and_graphics_cell.find_next('td').find('a')\n",
    "                    if actual_document_link is not None:\n",
    "                        actual_ten_k_document_href = actual_document_link['href']\n",
    "        #                 print(\"Documents href:\", document_href)\n",
    "                        actual_ten_k_document_link_base=\"https://www.sec.gov/\"\n",
    "                        actual_ten_k_document_link_tail=actual_ten_k_document_href\n",
    "                        actual_full_ten_k_document_link=actual_ten_k_document_link_base+actual_ten_k_document_link_tail\n",
    "                        actual_full_ten_k_document_link=x\n",
    "                        print(actual_full_ten_k_document_link)\n",
    "                        print(\"#########\")\n",
    "\n",
    "                        # Add the values to the DataFrame\n",
    "                        df_stock_top_us_plus_sec_titles.loc[df_stock_top_us_plus_sec_titles['title_searchable'] == one_title_searchable, 'ten_k_document_link'] = ten_k_document_link\n",
    "                        df_stock_top_us_plus_sec_titles.loc[df_stock_top_us_plus_sec_titles['title_searchable'] == one_title_searchable, 'actual_full_ten_k_document_link'] = actual_full_ten_k_document_link\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    finally:\n",
    "        # Close the page and browser\n",
    "        await ten_k_doc_link_page.close()\n",
    "        await ten_k_doc_link_browser.close()\n",
    "        await ten_k_doc_link_playwright.stop()\n",
    "\n",
    "        # Close the sec_home page and browser\n",
    "        await sec_home_page.close()\n",
    "        await sec_home_browser.close()\n",
    "        await sec_home_playwright.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b391493d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c448ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad40a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2426a923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15e8099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240c50a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_full_ten_k_document_link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802ef491",
   "metadata": {},
   "source": [
    "# Going into link and getting stock-based compensation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3416f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is final\n",
    "async def extracting_stock_compensation_for_one_company(one_title_searchable):\n",
    "\n",
    "    async def getting_10_k_link(one_title_searchable):\n",
    "        #### ON THE SEC HOME PAGE\n",
    "        # going to sec page\n",
    "        sec_home_playwright = await async_playwright().start()\n",
    "        sec_home_browser = await sec_home_playwright.chromium.launch(headless = False)\n",
    "        sec_home_page = await sec_home_browser.new_page()\n",
    "\n",
    "        await sec_home_page.goto(\"https://www.sec.gov/edgar/searchedgar/companysearch\")\n",
    "        time.sleep(6)\n",
    "\n",
    "        try:\n",
    "\n",
    "            # input company name (here is where we can enter title variable from df from csv)\n",
    "\n",
    "            await sec_home_page.fill(\"xpath=/html/body/div[1]/div/div/div/div[3]/div/div[2]/div[2]/div/div/article/div/div/div[1]/div[2]/div/div/div/div[1]/form/div[2]/input\", one_title_searchable)\n",
    "            time.sleep(6)\n",
    "\n",
    "            # click search\n",
    "\n",
    "            await sec_home_page.click(\"xpath=/html/body/div[1]/div/div/div/div[3]/div/div[2]/div[2]/div/div/article/div/div/div[1]/div[2]/div/div/div/div[1]/form/div[2]/button\")\n",
    "            time.sleep(6)\n",
    "\n",
    "            #============\n",
    "            # ON THE NEW PAGE RESULTING WITH ALL COMPANY DOCS\n",
    "            #input 10K for search\n",
    "            await sec_home_page.fill(\"xpath=/html/body/div[4]/div[2]/form/table/tbody/tr[3]/td[1]/input\", \"10-K\")\n",
    "            time.sleep(6)\n",
    "\n",
    "            # click search\n",
    "\n",
    "            await sec_home_page.click(\"xpath=/html/body/div[4]/div[2]/form/table/tbody/tr[3]/td[5]/input[1]\")\n",
    "            time.sleep(6)\n",
    "\n",
    "\n",
    "            #get all 10K entries\n",
    "            list_of_all_ten_k_entries_response= await sec_home_page.content()\n",
    "\n",
    "            list_of_all_ten_k_entries_doc = BeautifulSoup(list_of_all_ten_k_entries_response, 'html.parser')\n",
    "\n",
    "            #get the first 10K\n",
    "            list_of_all_ten_k_entries_table = list_of_all_ten_k_entries_doc.find('table', class_='tableFile2')\n",
    "\n",
    "            ten_k_entries_rows = list_of_all_ten_k_entries_table.find_all('tr')  # get the first row because it has the most recent 10K\n",
    "\n",
    "            most_recent_ten_k_link = None\n",
    "\n",
    "            for one_ten_k_entries_row in ten_k_entries_rows:\n",
    "                one_ten_k_entries_row_cells = one_ten_k_entries_row.find_all('td')  # Assuming each cell is represented by a <td> tag\n",
    "            #     print(one_ten_k_entries_row_cells)\n",
    "\n",
    "                for one_ten_k_entries_row_cell in one_ten_k_entries_row_cells:\n",
    "            #         print(one_ten_k_entries_row_cell)\n",
    "                    if one_ten_k_entries_row_cell.text.strip() == '10-K':\n",
    "                        ten_k_document_link_href = one_ten_k_entries_row_cell.find_next('td').find('a')['href']\n",
    "\n",
    "                        ten_k_document_link_base = \"https://www.sec.gov/\"\n",
    "                        ten_k_document_link_tail = ten_k_document_link_href\n",
    "                        ten_k_document_link = ten_k_document_link_base + ten_k_document_link_tail\n",
    "\n",
    "            #             print(document_link)\n",
    "\n",
    "                        # Store the first 10-K link and break out of the loop\n",
    "                        most_recent_ten_k_link = ten_k_document_link\n",
    "                        break\n",
    "\n",
    "                if most_recent_ten_k_link is not None:\n",
    "                    break\n",
    "\n",
    "\n",
    "            # ONCE I HAVE THE TEN K DOC LINK \n",
    "            # 3. Now on the page with the document and graphics table\n",
    "\n",
    "            ten_k_doc_link_playwright = await async_playwright().start()\n",
    "            ten_k_doc_link_browser = await ten_k_doc_link_playwright.chromium.launch(headless = False)\n",
    "            ten_k_doc_link_page = await ten_k_doc_link_browser.new_page()\n",
    "\n",
    "            time.sleep(6)\n",
    "\n",
    "            await ten_k_doc_link_page.goto(most_recent_ten_k_link)\n",
    "            time.sleep(6)\n",
    "\n",
    "\n",
    "\n",
    "            # Initiate the html and beautiful soup content \n",
    "\n",
    "            list_of_doc_and_graphics_response= await ten_k_doc_link_page.content()\n",
    "\n",
    "            list_of_doc_and_graphics_soup_doc = BeautifulSoup(list_of_doc_and_graphics_response, 'html.parser')\n",
    "\n",
    "            # get actual_full_ten_k_document_link\n",
    "            list_of_doc_and_graphics_table = list_of_doc_and_graphics_soup_doc.find('table', class_='tableFile')\n",
    "\n",
    "\n",
    "            list_of_doc_and_graphics_rows = list_of_doc_and_graphics_table.find_all('tr')  # Assuming each row is represented by a <tr> tag\n",
    "\n",
    "            for list_of_doc_and_graphics_row in list_of_doc_and_graphics_rows:\n",
    "                list_of_doc_and_graphics_cells = list_of_doc_and_graphics_row.find_all('td')  # Assuming each cell is represented by a <td> tag\n",
    "            #     print(cells)\n",
    "                for list_of_doc_and_graphics_cell in list_of_doc_and_graphics_cells:\n",
    "                    if list_of_doc_and_graphics_cell.text.strip() == '10-K':\n",
    "                        actual_document_link = list_of_doc_and_graphics_cell.find_next('td').find('a')\n",
    "                        if actual_document_link is not None:\n",
    "                            actual_ten_k_document_href = actual_document_link['href']\n",
    "            #                 print(\"Documents href:\", document_href)\n",
    "                            actual_ten_k_document_link_base=\"https://www.sec.gov/\"\n",
    "                            actual_ten_k_document_link_tail=actual_ten_k_document_href\n",
    "                            actual_full_ten_k_document_link=actual_ten_k_document_link_base+actual_ten_k_document_link_tail\n",
    "#                             global x\n",
    "                            x=actual_full_ten_k_document_link\n",
    "                            print(actual_full_ten_k_document_link)\n",
    "                            print(\"#########\")\n",
    "\n",
    "                            \n",
    "\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        finally:\n",
    "            # Close the page and browser\n",
    "            await ten_k_doc_link_page.close()\n",
    "            await ten_k_doc_link_browser.close()\n",
    "            await ten_k_doc_link_playwright.stop()\n",
    "\n",
    "            # Close the sec_home page and browser\n",
    "            await sec_home_page.close()\n",
    "            await sec_home_browser.close()\n",
    "            await sec_home_playwright.stop()\n",
    "            return actual_full_ten_k_document_link\n",
    "    \n",
    "    \n",
    "    \n",
    "    x=one_title_searchable\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # with change for link\n",
    "\n",
    "\n",
    "    list_of_actual_full_ten_k_document_links = [\"https://www.sec.gov/ix?doc=/Archives/edgar/data/789019/000156459022026876/msft-10k_20220630.htm\"]\n",
    "\n",
    "    actual_list_of_actual_full_ten_k_document_links = [\"https://www.sec.gov//ix?doc=/Archives/edgar/data/320193/000032019322000108/aapl-20220924.htm\",\"https://www.sec.gov/ix?doc=/Archives/edgar/data/789019/000156459022026876/msft-10k_20220630.htm\",\"https://www.sec.gov/ix?doc=/Archives/edgar/data/1652044/000165204423000016/goog-20221231.htm\"]\n",
    "\n",
    "    async def get_actual_full_ten_k_document_content(x):\n",
    "        playwright = await async_playwright().start()\n",
    "        browser = await playwright.chromium.launch(headless=False)\n",
    "    #     browser = await playwright.firefox.launch(headless=False)\n",
    "\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        await page.goto(x)\n",
    "        time.sleep(10)\n",
    "\n",
    "        while True:\n",
    "            # Get the number of spans on the page\n",
    "            spans_assigned = await page.query_selector_all(\"span\")\n",
    "            test = len(spans_assigned)\n",
    "\n",
    "            # Scroll in increments of 5000 pixels\n",
    "            await page.evaluate('''() => {\n",
    "                let distance = 5000;\n",
    "                window.scrollBy(0, distance);\n",
    "            }''')\n",
    "    #         await asyncio.sleep(3)\n",
    "\n",
    "            time.sleep(10)\n",
    "\n",
    "\n",
    "            spans_assigned_two = await page.query_selector_all(\"span\")\n",
    "            test2 = len(spans_assigned_two)\n",
    "            if test == test2:\n",
    "                break\n",
    "\n",
    "        # Get the content of the page\n",
    "        actual_full_ten_k_document_content = await page.content()\n",
    "        time.sleep(10)\n",
    "\n",
    "        await browser.close()\n",
    "        time.sleep(10)\n",
    "        await page.close()  # Add this line to close the page\n",
    "        time.sleep(10)\n",
    "\n",
    "        return actual_full_ten_k_document_content\n",
    "\n",
    "\n",
    "    async def process_actual_full_ten_k_document(actual_full_ten_k_document_content):\n",
    "        actual_full_ten_k_document_soup = BeautifulSoup(actual_full_ten_k_document_content, 'html.parser')\n",
    "        return actual_full_ten_k_document_soup\n",
    "\n",
    "\n",
    "    async def main():\n",
    "        dict_titles=['actual_full_ten_k_document_link_collection',\n",
    "                     'table_length',\n",
    "                     'list_of_stock_based_values',\n",
    "                     'first_stock_based_compensation',\n",
    "                     'second_stock_based_compensation',\n",
    "                     'third_stock_based_compensation']\n",
    "\n",
    "        possible_titles = ['CONSOLIDATED STATEMENTS OF CASH FLOWS',\n",
    "                           'CASH FLOWS STATEMENTS',\n",
    "                           'Consolidated Statements of Cash Flows',\n",
    "                           'Consolidated Statement of Cash Flows',\n",
    "                          'CONSOLIDATED STATEMENT OF CASH FLOWS',\n",
    "                          'STATEMENTS OF CONSOLIDATED CASH FLOWS',\n",
    "                          'Consolidated Cash Flow Statement',\n",
    "                          'Consolidated Statement of Cash Flow for the Years Ended December 31',\n",
    "                          'STATEMENT OF CASH FLOWS',\n",
    "                          'CONSOLIDATED STATEMENTS OF CASH FLOW',\n",
    "                          'Statements of Consolidated Cash Flows']\n",
    "\n",
    "        stock_related_texts = [\n",
    "            'Stock-based compensation expense',\n",
    "            'Stock-based compensation expense',\n",
    "            'Share-based compensation expense',\n",
    "            'Stock-based compensation',\n",
    "            'Share-based compensation'\n",
    "        ]\n",
    "\n",
    "        all_ten_k_document_lists = []\n",
    "\n",
    "\n",
    "    #     list_of_example_actual_full_ten_k_document_soup_doc = []\n",
    "\n",
    "        # Define the group size\n",
    "        group_size = 2\n",
    "        items=[x]\n",
    "\n",
    "        # Iterate over groups of items\n",
    "        for start_index in range(0, len(items), group_size):\n",
    "            group_items = items[start_index:start_index + group_size]\n",
    "\n",
    "            list_of_example_actual_full_ten_k_document_soup_doc = []\n",
    "\n",
    "\n",
    "    #         # Iterate over items within the group\n",
    "    #         for item in group_items:\n",
    "    #             print(item)\n",
    "\n",
    "    #         # Add a separator for each group\n",
    "    #         print(\"Group separator\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            url_to_parsed_content = {}\n",
    "\n",
    "\n",
    "            for actual_full_ten_k_document_link in group_items:\n",
    "                try:\n",
    "                    actual_full_ten_k_document_link_collection = actual_full_ten_k_document_link\n",
    "                    print(actual_full_ten_k_document_link)\n",
    "\n",
    "                    for _ in range(3):\n",
    "                        try:\n",
    "                            actual_full_ten_k_document_content = await get_actual_full_ten_k_document_content(actual_full_ten_k_document_link)\n",
    "                            example_actual_full_ten_k_document_soup_doc = await process_actual_full_ten_k_document(actual_full_ten_k_document_content)\n",
    "                            list_of_example_actual_full_ten_k_document_soup_doc.append(example_actual_full_ten_k_document_soup_doc)\n",
    "                                # Store the parsed content in the dictionary.\n",
    "                            url_to_parsed_content[actual_full_ten_k_document_link] = example_actual_full_ten_k_document_soup_doc\n",
    "\n",
    "                            break  # Break the loop if successful\n",
    "                        except Exception as e:\n",
    "                            print(f\"An error occurred: {str(e)}. Retrying...\")\n",
    "                            time.sleep(15)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            for actual_full_ten_k_document_link, one_actual_full_ten_k_document_soup_doc in url_to_parsed_content.items():\n",
    "                ten_k_detail_list=[]\n",
    "\n",
    "                if one_actual_full_ten_k_document_soup_doc:\n",
    "\n",
    "                    elements = one_actual_full_ten_k_document_soup_doc.find_all(text=possible_titles)\n",
    "\n",
    "                    if not elements:\n",
    "                        print(\"Titles not found in the HTML.\")\n",
    "                        continue\n",
    "\n",
    "                    for element in elements:\n",
    "                        if element.strip() in possible_titles:\n",
    "                            following_elements = element.find_all_next()\n",
    "                            table = None\n",
    "\n",
    "                            # Iterate through following elements and stop when a table is found or after three components\n",
    "                            component_count = 0\n",
    "                            for next_element in following_elements:\n",
    "                                if next_element.name == 'table' and any(text in next_element.get_text() for text in stock_related_texts):\n",
    "                                    table = next_element\n",
    "                                    break\n",
    "                                elif next_element.name in ['p', 'span']:\n",
    "                                    component_count += 1\n",
    "                                    if component_count >= 5:\n",
    "                                        break\n",
    "\n",
    "                            if table is None:\n",
    "                                tables = one_actual_full_ten_k_document_soup_doc.find_all(\"table\")\n",
    "                                for t in tables:     \n",
    "                                    if t:\n",
    "                                        table_text = t.get_text()\n",
    "                                        if any(title in table_text for title in possible_titles) and any(stock_related_text in table_text for stock_related_text in stock_related_texts):\n",
    "                                            rows = t.find_all('tr')\n",
    "                                            table_length = len(rows)\n",
    "                                            print(f\"Table Length from within: {len(rows)}\") \n",
    "                                        break\n",
    "\n",
    "                            if table:\n",
    "                                rows = table.find_all('tr')\n",
    "                                table_length=len(rows)\n",
    "\n",
    "                                print(f\"Table Length: {len(rows)}\")                        \n",
    "\n",
    "                                break\n",
    "\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        print(\"No table found within or after the possible titles in the HTML.\")\n",
    "\n",
    "                else:\n",
    "                    print(\"Document not processed, skipping\")\n",
    "\n",
    "\n",
    "                if table:\n",
    "                    target_td = table.find('td', text=stock_related_texts)\n",
    "                else:\n",
    "                    target_td = None\n",
    "\n",
    "                if target_td:\n",
    "                    # Move up to the <tr> containing the <td> with \"share-based compensation\"\n",
    "                    target_tr = target_td.find_parent('tr')\n",
    "\n",
    "                    # Extract all the <td> values from the <tr> and put them in a column\n",
    "                    values_in_column = [td.get_text(strip=True) for td in target_tr.find_all('td')]\n",
    "\n",
    "                    # Output the values in the column\n",
    "                    if values_in_column:\n",
    "                        stock_stock_based_compensation_title=values_in_column[0]\n",
    "                        list_of_stock_based_values=values_in_column\n",
    "                        numbers_only_values_in_column = [entry for entry in values_in_column if any(char.isdigit() for char in entry)]\n",
    "                        first_stock_based_compensation=numbers_only_values_in_column[0]\n",
    "                        second_stock_based_compensation=numbers_only_values_in_column[1]\n",
    "                        third_stock_based_compensation=numbers_only_values_in_column[2]\n",
    "                        for value in values_in_column:\n",
    "                            print(value)\n",
    "\n",
    "                    ten_k_detail_list.append(actual_full_ten_k_document_link)\n",
    "                    ten_k_detail_list.append(table_length)\n",
    "                    ten_k_detail_list.append(list_of_stock_based_values)\n",
    "                    ten_k_detail_list.append(first_stock_based_compensation)\n",
    "                    ten_k_detail_list.append(second_stock_based_compensation)\n",
    "                    ten_k_detail_list.append(third_stock_based_compensation)\n",
    "\n",
    "                    first_rep = dict(zip(dict_titles, ten_k_detail_list))\n",
    "                    all_ten_k_document_lists.append(first_rep)\n",
    "\n",
    "\n",
    "\n",
    "                else:\n",
    "                    print(\"The 'Share-Based Compensation' row not found!\")\n",
    "                time.sleep(10)\n",
    "\n",
    "    #     print(all_ten_k_document_lists)\n",
    "        return all_ten_k_document_lists\n",
    "\n",
    "\n",
    "    # Run the main coroutine\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    # Use asyncio.run() to run the main coroutine\n",
    "    all_ten_k_document_lists_of_dicts=asyncio.run(main())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9102c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "await extracting_stock_compensation_for_one_company(\"https://www.sec.gov//ix?doc=/Archives/edgar/data/320193/000032019322000108/aapl-20220924.htm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fc4ea0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dca5602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51c1689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a59a53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07b8259",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592e7e21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99c3837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5668d777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebedc165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545d0331",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefeb5f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5425cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc7b0f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae131b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is final\n",
    "async def extracting_stock_compensation_for_one_company(one_title_searchable):\n",
    "\n",
    "    async def getting_10_k_link(one_title_searchable):\n",
    "        #### ON THE SEC HOME PAGE\n",
    "        # going to sec page\n",
    "        sec_home_playwright = await async_playwright().start()\n",
    "        sec_home_browser = await sec_home_playwright.chromium.launch(headless = False)\n",
    "        sec_home_page = await sec_home_browser.new_page()\n",
    "\n",
    "        await sec_home_page.goto(\"https://www.sec.gov/edgar/searchedgar/companysearch\")\n",
    "        time.sleep(6)\n",
    "\n",
    "        try:\n",
    "\n",
    "            # input company name (here is where we can enter title variable from df from csv)\n",
    "\n",
    "            await sec_home_page.fill(\"xpath=/html/body/div[1]/div/div/div/div[3]/div/div[2]/div[2]/div/div/article/div/div/div[1]/div[2]/div/div/div/div[1]/form/div[2]/input\", one_title_searchable)\n",
    "            time.sleep(6)\n",
    "\n",
    "            # click search\n",
    "\n",
    "            await sec_home_page.click(\"xpath=/html/body/div[1]/div/div/div/div[3]/div/div[2]/div[2]/div/div/article/div/div/div[1]/div[2]/div/div/div/div[1]/form/div[2]/button\")\n",
    "            time.sleep(6)\n",
    "\n",
    "            #============\n",
    "            # ON THE NEW PAGE RESULTING WITH ALL COMPANY DOCS\n",
    "            #input 10K for search\n",
    "            await sec_home_page.fill(\"xpath=/html/body/div[4]/div[2]/form/table/tbody/tr[3]/td[1]/input\", \"10-K\")\n",
    "            time.sleep(6)\n",
    "\n",
    "            # click search\n",
    "\n",
    "            await sec_home_page.click(\"xpath=/html/body/div[4]/div[2]/form/table/tbody/tr[3]/td[5]/input[1]\")\n",
    "            time.sleep(6)\n",
    "\n",
    "\n",
    "            #get all 10K entries\n",
    "            list_of_all_ten_k_entries_response= await sec_home_page.content()\n",
    "\n",
    "            list_of_all_ten_k_entries_doc = BeautifulSoup(list_of_all_ten_k_entries_response, 'html.parser')\n",
    "\n",
    "            #get the first 10K\n",
    "            list_of_all_ten_k_entries_table = list_of_all_ten_k_entries_doc.find('table', class_='tableFile2')\n",
    "\n",
    "            ten_k_entries_rows = list_of_all_ten_k_entries_table.find_all('tr')  # get the first row because it has the most recent 10K\n",
    "\n",
    "            most_recent_ten_k_link = None\n",
    "\n",
    "            for one_ten_k_entries_row in ten_k_entries_rows:\n",
    "                one_ten_k_entries_row_cells = one_ten_k_entries_row.find_all('td')  # Assuming each cell is represented by a <td> tag\n",
    "            #     print(one_ten_k_entries_row_cells)\n",
    "\n",
    "                for one_ten_k_entries_row_cell in one_ten_k_entries_row_cells:\n",
    "            #         print(one_ten_k_entries_row_cell)\n",
    "                    if one_ten_k_entries_row_cell.text.strip() == '10-K':\n",
    "                        ten_k_document_link_href = one_ten_k_entries_row_cell.find_next('td').find('a')['href']\n",
    "\n",
    "                        ten_k_document_link_base = \"https://www.sec.gov/\"\n",
    "                        ten_k_document_link_tail = ten_k_document_link_href\n",
    "                        ten_k_document_link = ten_k_document_link_base + ten_k_document_link_tail\n",
    "\n",
    "            #             print(document_link)\n",
    "\n",
    "                        # Store the first 10-K link and break out of the loop\n",
    "                        most_recent_ten_k_link = ten_k_document_link\n",
    "                        break\n",
    "\n",
    "                if most_recent_ten_k_link is not None:\n",
    "                    break\n",
    "\n",
    "\n",
    "            # ONCE I HAVE THE TEN K DOC LINK \n",
    "            # 3. Now on the page with the document and graphics table\n",
    "\n",
    "            ten_k_doc_link_playwright = await async_playwright().start()\n",
    "            ten_k_doc_link_browser = await ten_k_doc_link_playwright.chromium.launch(headless = False)\n",
    "            ten_k_doc_link_page = await ten_k_doc_link_browser.new_page()\n",
    "\n",
    "            time.sleep(6)\n",
    "\n",
    "            await ten_k_doc_link_page.goto(most_recent_ten_k_link)\n",
    "            time.sleep(6)\n",
    "\n",
    "\n",
    "\n",
    "            # Initiate the html and beautiful soup content \n",
    "\n",
    "            list_of_doc_and_graphics_response= await ten_k_doc_link_page.content()\n",
    "\n",
    "            list_of_doc_and_graphics_soup_doc = BeautifulSoup(list_of_doc_and_graphics_response, 'html.parser')\n",
    "\n",
    "            # get actual_full_ten_k_document_link\n",
    "            list_of_doc_and_graphics_table = list_of_doc_and_graphics_soup_doc.find('table', class_='tableFile')\n",
    "\n",
    "\n",
    "            list_of_doc_and_graphics_rows = list_of_doc_and_graphics_table.find_all('tr')  # Assuming each row is represented by a <tr> tag\n",
    "\n",
    "            for list_of_doc_and_graphics_row in list_of_doc_and_graphics_rows:\n",
    "                list_of_doc_and_graphics_cells = list_of_doc_and_graphics_row.find_all('td')  # Assuming each cell is represented by a <td> tag\n",
    "            #     print(cells)\n",
    "                for list_of_doc_and_graphics_cell in list_of_doc_and_graphics_cells:\n",
    "                    if list_of_doc_and_graphics_cell.text.strip() == '10-K':\n",
    "                        actual_document_link = list_of_doc_and_graphics_cell.find_next('td').find('a')\n",
    "                        if actual_document_link is not None:\n",
    "                            actual_ten_k_document_href = actual_document_link['href']\n",
    "            #                 print(\"Documents href:\", document_href)\n",
    "                            actual_ten_k_document_link_base=\"https://www.sec.gov/\"\n",
    "                            actual_ten_k_document_link_tail=actual_ten_k_document_href\n",
    "                            actual_full_ten_k_document_link=actual_ten_k_document_link_base+actual_ten_k_document_link_tail\n",
    "                            x=actual_full_ten_k_document_link\n",
    "                            print(actual_full_ten_k_document_link)\n",
    "                            print(\"#########\")\n",
    "\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        finally:\n",
    "            # Close the page and browser\n",
    "            await ten_k_doc_link_page.close()\n",
    "            await ten_k_doc_link_browser.close()\n",
    "            await ten_k_doc_link_playwright.stop()\n",
    "\n",
    "            # Close the sec_home page and browser\n",
    "            await sec_home_page.close()\n",
    "            await sec_home_browser.close()\n",
    "            await sec_home_playwright.stop()\n",
    "        return x\n",
    "\n",
    "    # with change for link\n",
    "\n",
    "\n",
    "    list_of_actual_full_ten_k_document_links = [\"https://www.sec.gov/ix?doc=/Archives/edgar/data/789019/000156459022026876/msft-10k_20220630.htm\"]\n",
    "\n",
    "    actual_list_of_actual_full_ten_k_document_links = [\"https://www.sec.gov//ix?doc=/Archives/edgar/data/320193/000032019322000108/aapl-20220924.htm\",\"https://www.sec.gov/ix?doc=/Archives/edgar/data/789019/000156459022026876/msft-10k_20220630.htm\",\"https://www.sec.gov/ix?doc=/Archives/edgar/data/1652044/000165204423000016/goog-20221231.htm\"]\n",
    "\n",
    "    async def get_actual_full_ten_k_document_content(x):\n",
    "        playwright = await async_playwright().start()\n",
    "        browser = await playwright.chromium.launch(headless=False)\n",
    "    #     browser = await playwright.firefox.launch(headless=False)\n",
    "\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        await page.goto(x)\n",
    "        time.sleep(10)\n",
    "\n",
    "        while True:\n",
    "            # Get the number of spans on the page\n",
    "            spans_assigned = await page.query_selector_all(\"span\")\n",
    "            test = len(spans_assigned)\n",
    "\n",
    "            # Scroll in increments of 5000 pixels\n",
    "            await page.evaluate('''() => {\n",
    "                let distance = 5000;\n",
    "                window.scrollBy(0, distance);\n",
    "            }''')\n",
    "    #         await asyncio.sleep(3)\n",
    "\n",
    "            time.sleep(10)\n",
    "\n",
    "\n",
    "            spans_assigned_two = await page.query_selector_all(\"span\")\n",
    "            test2 = len(spans_assigned_two)\n",
    "            if test == test2:\n",
    "                break\n",
    "\n",
    "        # Get the content of the page\n",
    "        actual_full_ten_k_document_content = await page.content()\n",
    "        time.sleep(10)\n",
    "\n",
    "        await browser.close()\n",
    "        time.sleep(10)\n",
    "        await page.close()  # Add this line to close the page\n",
    "        time.sleep(10)\n",
    "\n",
    "        return actual_full_ten_k_document_content\n",
    "\n",
    "\n",
    "    async def process_actual_full_ten_k_document(actual_full_ten_k_document_content):\n",
    "        actual_full_ten_k_document_soup = BeautifulSoup(actual_full_ten_k_document_content, 'html.parser')\n",
    "        return actual_full_ten_k_document_soup\n",
    "\n",
    "\n",
    "    async def main():\n",
    "        dict_titles=['actual_full_ten_k_document_link_collection',\n",
    "                     'table_length',\n",
    "                     'list_of_stock_based_values',\n",
    "                     'first_stock_based_compensation',\n",
    "                     'second_stock_based_compensation',\n",
    "                     'third_stock_based_compensation']\n",
    "\n",
    "        possible_titles = ['CONSOLIDATED STATEMENTS OF CASH FLOWS',\n",
    "                           'CASH FLOWS STATEMENTS',\n",
    "                           'Consolidated Statements of Cash Flows',\n",
    "                           'Consolidated Statement of Cash Flows',\n",
    "                          'CONSOLIDATED STATEMENT OF CASH FLOWS',\n",
    "                          'STATEMENTS OF CONSOLIDATED CASH FLOWS',\n",
    "                          'Consolidated Cash Flow Statement',\n",
    "                          'Consolidated Statement of Cash Flow for the Years Ended December 31',\n",
    "                          'STATEMENT OF CASH FLOWS',\n",
    "                          'CONSOLIDATED STATEMENTS OF CASH FLOW',\n",
    "                          'Statements of Consolidated Cash Flows']\n",
    "\n",
    "        stock_related_texts = [\n",
    "            'Stock-based compensation expense',\n",
    "            'Stock-based compensation expense',\n",
    "            'Share-based compensation expense',\n",
    "            'Stock-based compensation',\n",
    "            'Share-based compensation'\n",
    "        ]\n",
    "\n",
    "        all_ten_k_document_lists = []\n",
    "\n",
    "\n",
    "    #     list_of_example_actual_full_ten_k_document_soup_doc = []\n",
    "\n",
    "        # Define the group size\n",
    "        group_size = 2\n",
    "        items=[x]\n",
    "\n",
    "        # Iterate over groups of items\n",
    "        for start_index in range(0, len(items), group_size):\n",
    "            group_items = items[start_index:start_index + group_size]\n",
    "\n",
    "            list_of_example_actual_full_ten_k_document_soup_doc = []\n",
    "\n",
    "\n",
    "    #         # Iterate over items within the group\n",
    "    #         for item in group_items:\n",
    "    #             print(item)\n",
    "\n",
    "    #         # Add a separator for each group\n",
    "    #         print(\"Group separator\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            url_to_parsed_content = {}\n",
    "\n",
    "\n",
    "            for actual_full_ten_k_document_link in group_items:\n",
    "                try:\n",
    "                    actual_full_ten_k_document_link_collection = actual_full_ten_k_document_link\n",
    "                    print(actual_full_ten_k_document_link)\n",
    "\n",
    "                    for _ in range(3):\n",
    "                        try:\n",
    "                            actual_full_ten_k_document_content = await get_actual_full_ten_k_document_content(actual_full_ten_k_document_link)\n",
    "                            example_actual_full_ten_k_document_soup_doc = await process_actual_full_ten_k_document(actual_full_ten_k_document_content)\n",
    "                            list_of_example_actual_full_ten_k_document_soup_doc.append(example_actual_full_ten_k_document_soup_doc)\n",
    "                                # Store the parsed content in the dictionary.\n",
    "                            url_to_parsed_content[actual_full_ten_k_document_link] = example_actual_full_ten_k_document_soup_doc\n",
    "\n",
    "                            break  # Break the loop if successful\n",
    "                        except Exception as e:\n",
    "                            print(f\"An error occurred: {str(e)}. Retrying...\")\n",
    "                            time.sleep(15)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            for actual_full_ten_k_document_link, one_actual_full_ten_k_document_soup_doc in url_to_parsed_content.items():\n",
    "                ten_k_detail_list=[]\n",
    "\n",
    "                elements = one_actual_full_ten_k_document_soup_doc.find_all(text=possible_titles)\n",
    "\n",
    "                if not elements:\n",
    "                    print(\"Titles not found in the HTML.\")\n",
    "                    continue\n",
    "\n",
    "                for element in elements:\n",
    "                    if element.strip() in possible_titles:\n",
    "                        following_elements = element.find_all_next()\n",
    "                        table = None\n",
    "\n",
    "                        # Iterate through following elements and stop when a table is found or after three components\n",
    "                        component_count = 0\n",
    "                        for next_element in following_elements:\n",
    "                            if next_element.name == 'table' and any(text in next_element.get_text() for text in stock_related_texts):\n",
    "                                table = next_element\n",
    "                                break\n",
    "                            elif next_element.name in ['p', 'span']:\n",
    "                                component_count += 1\n",
    "                                if component_count >= 5:\n",
    "                                    break\n",
    "\n",
    "                        if table:\n",
    "                            rows = table.find_all('tr')\n",
    "                            table_length=len(rows)\n",
    "\n",
    "                            print(f\"Table Length: {len(rows)}\")                        \n",
    "\n",
    "                            break\n",
    "\n",
    "                else:\n",
    "                    print(\"No table found after the possible titles in the HTML.\")\n",
    "\n",
    "                if table:\n",
    "                    target_td = table.find('td', text=stock_related_texts)\n",
    "                else:\n",
    "                    target_td = None\n",
    "\n",
    "                if target_td:\n",
    "                    # Move up to the <tr> containing the <td> with \"share-based compensation\"\n",
    "                    target_tr = target_td.find_parent('tr')\n",
    "\n",
    "                    # Extract all the <td> values from the <tr> and put them in a column\n",
    "                    values_in_column = [td.get_text(strip=True) for td in target_tr.find_all('td')]\n",
    "\n",
    "                    # Output the values in the column\n",
    "                    if values_in_column:\n",
    "                        stock_stock_based_compensation_title=values_in_column[0]\n",
    "                        list_of_stock_based_values=values_in_column\n",
    "                        numbers_only_values_in_column = [entry for entry in values_in_column if any(char.isdigit() for char in entry)]\n",
    "                        first_stock_based_compensation=numbers_only_values_in_column[0]\n",
    "                        second_stock_based_compensation=numbers_only_values_in_column[1]\n",
    "                        third_stock_based_compensation=numbers_only_values_in_column[2]\n",
    "                        for value in values_in_column:\n",
    "                            print(value)\n",
    "\n",
    "                    ten_k_detail_list.append(actual_full_ten_k_document_link)\n",
    "                    ten_k_detail_list.append(table_length)\n",
    "                    ten_k_detail_list.append(list_of_stock_based_values)\n",
    "                    ten_k_detail_list.append(first_stock_based_compensation)\n",
    "                    ten_k_detail_list.append(second_stock_based_compensation)\n",
    "                    ten_k_detail_list.append(third_stock_based_compensation)\n",
    "\n",
    "                    first_rep = dict(zip(dict_titles, ten_k_detail_list))\n",
    "                    all_ten_k_document_lists.append(first_rep)\n",
    "\n",
    "\n",
    "\n",
    "                else:\n",
    "                    print(\"The 'Share-Based Compensation' row not found!\")\n",
    "                time.sleep(10)\n",
    "\n",
    "    #     print(all_ten_k_document_lists)\n",
    "        return all_ten_k_document_lists\n",
    "\n",
    "\n",
    "    # Run the main coroutine\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    # Use asyncio.run() to run the main coroutine\n",
    "    all_ten_k_document_lists_of_dicts=asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a068861",
   "metadata": {},
   "outputs": [],
   "source": [
    "await extracting_stock_compensation_for_one_company(\"https://www.sec.gov//ix?doc=/Archives/edgar/data/320193/000032019322000108/aapl-20220924.htm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3d7bff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61607405",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb0f1ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c768528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0695dd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164d5d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with change for link\n",
    "x=actual_full_ten_k_document_link\n",
    "\n",
    "list_of_actual_full_ten_k_document_links = [\"https://www.sec.gov/ix?doc=/Archives/edgar/data/789019/000156459022026876/msft-10k_20220630.htm\"]\n",
    "\n",
    "actual_list_of_actual_full_ten_k_document_links = [\"https://www.sec.gov//ix?doc=/Archives/edgar/data/320193/000032019322000108/aapl-20220924.htm\",\"https://www.sec.gov/ix?doc=/Archives/edgar/data/789019/000156459022026876/msft-10k_20220630.htm\",\"https://www.sec.gov/ix?doc=/Archives/edgar/data/1652044/000165204423000016/goog-20221231.htm\"]\n",
    "\n",
    "async def get_actual_full_ten_k_document_content(x):\n",
    "    playwright = await async_playwright().start()\n",
    "    browser = await playwright.chromium.launch(headless=False)\n",
    "#     browser = await playwright.firefox.launch(headless=False)\n",
    "\n",
    "    page = await browser.new_page()\n",
    "\n",
    "    await page.goto(x)\n",
    "    time.sleep(10)\n",
    "\n",
    "    while True:\n",
    "        # Get the number of spans on the page\n",
    "        spans_assigned = await page.query_selector_all(\"span\")\n",
    "        test = len(spans_assigned)\n",
    "\n",
    "        # Scroll in increments of 5000 pixels\n",
    "        await page.evaluate('''() => {\n",
    "            let distance = 5000;\n",
    "            window.scrollBy(0, distance);\n",
    "        }''')\n",
    "#         await asyncio.sleep(3)\n",
    "\n",
    "        time.sleep(10)\n",
    "\n",
    "\n",
    "        spans_assigned_two = await page.query_selector_all(\"span\")\n",
    "        test2 = len(spans_assigned_two)\n",
    "        if test == test2:\n",
    "            break\n",
    "\n",
    "    # Get the content of the page\n",
    "    actual_full_ten_k_document_content = await page.content()\n",
    "    time.sleep(10)\n",
    "\n",
    "    await browser.close()\n",
    "    time.sleep(10)\n",
    "    await page.close()  # Add this line to close the page\n",
    "    time.sleep(10)\n",
    "    \n",
    "    return actual_full_ten_k_document_content\n",
    "\n",
    "\n",
    "async def process_actual_full_ten_k_document(actual_full_ten_k_document_content):\n",
    "    actual_full_ten_k_document_soup = BeautifulSoup(actual_full_ten_k_document_content, 'html.parser')\n",
    "    return actual_full_ten_k_document_soup\n",
    "\n",
    "\n",
    "async def main():\n",
    "    dict_titles=['actual_full_ten_k_document_link_collection',\n",
    "                 'table_length',\n",
    "                 'list_of_stock_based_values',\n",
    "                 'first_stock_based_compensation',\n",
    "                 'second_stock_based_compensation',\n",
    "                 'third_stock_based_compensation']\n",
    "    \n",
    "    possible_titles = ['CONSOLIDATED STATEMENTS OF CASH FLOWS',\n",
    "                       'CASH FLOWS STATEMENTS',\n",
    "                       'Consolidated Statements of Cash Flows',\n",
    "                       'Consolidated Statement of Cash Flows',\n",
    "                      'CONSOLIDATED STATEMENT OF CASH FLOWS',\n",
    "                      'STATEMENTS OF CONSOLIDATED CASH FLOWS',\n",
    "                      'Consolidated Cash Flow Statement',\n",
    "                      'Consolidated Statement of Cash Flow for the Years Ended December 31',\n",
    "                      'STATEMENT OF CASH FLOWS',\n",
    "                      'CONSOLIDATED STATEMENTS OF CASH FLOW',\n",
    "                      'Statements of Consolidated Cash Flows']\n",
    "\n",
    "    stock_related_texts = [\n",
    "        'Stock-based compensation expense',\n",
    "        'Stock-based compensation expense',\n",
    "        'Share-based compensation expense',\n",
    "        'Stock-based compensation',\n",
    "        'Share-based compensation'\n",
    "    ]\n",
    "\n",
    "    all_ten_k_document_lists = []\n",
    "\n",
    "\n",
    "#     list_of_example_actual_full_ten_k_document_soup_doc = []\n",
    "\n",
    "    # Define the group size\n",
    "    group_size = 1\n",
    "    items=[x]\n",
    "    \n",
    "    # Iterate over groups of items\n",
    "    for start_index in range(0, len(items), group_size):\n",
    "        group_items = items[start_index:start_index + group_size]\n",
    "        \n",
    "        list_of_example_actual_full_ten_k_document_soup_doc = []\n",
    "\n",
    "\n",
    "#         # Iterate over items within the group\n",
    "#         for item in group_items:\n",
    "#             print(item)\n",
    "\n",
    "#         # Add a separator for each group\n",
    "#         print(\"Group separator\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        url_to_parsed_content = {}\n",
    "\n",
    "\n",
    "        for actual_full_ten_k_document_link in group_items:\n",
    "            try:\n",
    "                actual_full_ten_k_document_link_collection = actual_full_ten_k_document_link\n",
    "                print(actual_full_ten_k_document_link)\n",
    "\n",
    "                for _ in range(3):\n",
    "                    try:\n",
    "                        actual_full_ten_k_document_content = await get_actual_full_ten_k_document_content(actual_full_ten_k_document_link)\n",
    "                        example_actual_full_ten_k_document_soup_doc = await process_actual_full_ten_k_document(actual_full_ten_k_document_content)\n",
    "                        list_of_example_actual_full_ten_k_document_soup_doc.append(example_actual_full_ten_k_document_soup_doc)\n",
    "                            # Store the parsed content in the dictionary.\n",
    "                        url_to_parsed_content[actual_full_ten_k_document_link] = example_actual_full_ten_k_document_soup_doc\n",
    "\n",
    "                        break  # Break the loop if successful\n",
    "                    except Exception as e:\n",
    "                        print(f\"An error occurred: {str(e)}. Retrying...\")\n",
    "                        time.sleep(15)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        for actual_full_ten_k_document_link, one_actual_full_ten_k_document_soup_doc in url_to_parsed_content.items():\n",
    "            ten_k_detail_list=[]\n",
    "\n",
    "            elements = one_actual_full_ten_k_document_soup_doc.find_all(text=possible_titles)\n",
    "\n",
    "            if not elements:\n",
    "                print(\"Titles not found in the HTML.\")\n",
    "                continue\n",
    "\n",
    "            for element in elements:\n",
    "                if element.strip() in possible_titles:\n",
    "                    following_elements = element.find_all_next()\n",
    "                    table = None\n",
    "\n",
    "                    # Iterate through following elements and stop when a table is found or after three components\n",
    "                    component_count = 0\n",
    "                    for next_element in following_elements:\n",
    "                        if next_element.name == 'table' and any(text in next_element.get_text() for text in stock_related_texts):\n",
    "                            table = next_element\n",
    "                            break\n",
    "                        elif next_element.name in ['p', 'span']:\n",
    "                            component_count += 1\n",
    "                            if component_count >= 5:\n",
    "                                break\n",
    "\n",
    "                    if table:\n",
    "                        rows = table.find_all('tr')\n",
    "                        table_length=len(rows)\n",
    "\n",
    "                        print(f\"Table Length: {len(rows)}\")                        \n",
    "\n",
    "                        break\n",
    "\n",
    "            else:\n",
    "                print(\"No table found after the possible titles in the HTML.\")\n",
    "\n",
    "            if table:\n",
    "                target_td = table.find('td', text=stock_related_texts)\n",
    "            else:\n",
    "                target_td = None\n",
    "\n",
    "            if target_td:\n",
    "                # Move up to the <tr> containing the <td> with \"share-based compensation\"\n",
    "                target_tr = target_td.find_parent('tr')\n",
    "\n",
    "                # Extract all the <td> values from the <tr> and put them in a column\n",
    "                values_in_column = [td.get_text(strip=True) for td in target_tr.find_all('td')]\n",
    "\n",
    "                # Output the values in the column\n",
    "                if values_in_column:\n",
    "                    stock_stock_based_compensation_title=values_in_column[0]\n",
    "                    list_of_stock_based_values=values_in_column\n",
    "                    numbers_only_values_in_column = [entry for entry in values_in_column if any(char.isdigit() for char in entry)]\n",
    "                    first_stock_based_compensation=numbers_only_values_in_column[0]\n",
    "                    second_stock_based_compensation=numbers_only_values_in_column[1]\n",
    "                    third_stock_based_compensation=numbers_only_values_in_column[2]\n",
    "                    for value in values_in_column:\n",
    "                        print(value)\n",
    "\n",
    "                ten_k_detail_list.append(actual_full_ten_k_document_link)\n",
    "                ten_k_detail_list.append(table_length)\n",
    "                ten_k_detail_list.append(list_of_stock_based_values)\n",
    "                ten_k_detail_list.append(first_stock_based_compensation)\n",
    "                ten_k_detail_list.append(second_stock_based_compensation)\n",
    "                ten_k_detail_list.append(third_stock_based_compensation)\n",
    "\n",
    "                first_rep = dict(zip(dict_titles, ten_k_detail_list))\n",
    "                all_ten_k_document_lists.append(first_rep)\n",
    "\n",
    "\n",
    "\n",
    "            else:\n",
    "                print(\"The 'Share-Based Compensation' row not found!\")\n",
    "            time.sleep(10)\n",
    "\n",
    "#     print(all_ten_k_document_lists)\n",
    "    return all_ten_k_document_lists\n",
    "\n",
    "\n",
    "# Run the main coroutine\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Use asyncio.run() to run the main coroutine\n",
    "all_ten_k_document_lists_of_dicts=asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87daf9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e6d168",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is final\n",
    "async def extracting_stock_compensation_for_one_company(one_title_searchable):\n",
    "\n",
    "    async def getting_10_k_link(one_title_searchable):\n",
    "        #### ON THE SEC HOME PAGE\n",
    "        # going to sec page\n",
    "        sec_home_playwright = await async_playwright().start()\n",
    "        sec_home_browser = await sec_home_playwright.chromium.launch(headless = False)\n",
    "        sec_home_page = await sec_home_browser.new_page()\n",
    "\n",
    "        await sec_home_page.goto(\"https://www.sec.gov/edgar/searchedgar/companysearch\")\n",
    "        time.sleep(6)\n",
    "\n",
    "        try:\n",
    "\n",
    "            # input company name (here is where we can enter title variable from df from csv)\n",
    "\n",
    "            await sec_home_page.fill(\"xpath=/html/body/div[1]/div/div/div/div[3]/div/div[2]/div[2]/div/div/article/div/div/div[1]/div[2]/div/div/div/div[1]/form/div[2]/input\", one_title_searchable)\n",
    "            time.sleep(6)\n",
    "\n",
    "            # click search\n",
    "\n",
    "            await sec_home_page.click(\"xpath=/html/body/div[1]/div/div/div/div[3]/div/div[2]/div[2]/div/div/article/div/div/div[1]/div[2]/div/div/div/div[1]/form/div[2]/button\")\n",
    "            time.sleep(6)\n",
    "\n",
    "            #============\n",
    "            # ON THE NEW PAGE RESULTING WITH ALL COMPANY DOCS\n",
    "            #input 10K for search\n",
    "            await sec_home_page.fill(\"xpath=/html/body/div[4]/div[2]/form/table/tbody/tr[3]/td[1]/input\", \"10-K\")\n",
    "            time.sleep(6)\n",
    "\n",
    "            # click search\n",
    "\n",
    "            await sec_home_page.click(\"xpath=/html/body/div[4]/div[2]/form/table/tbody/tr[3]/td[5]/input[1]\")\n",
    "            time.sleep(6)\n",
    "\n",
    "\n",
    "            #get all 10K entries\n",
    "            list_of_all_ten_k_entries_response= await sec_home_page.content()\n",
    "\n",
    "            list_of_all_ten_k_entries_doc = BeautifulSoup(list_of_all_ten_k_entries_response, 'html.parser')\n",
    "\n",
    "            #get the first 10K\n",
    "            list_of_all_ten_k_entries_table = list_of_all_ten_k_entries_doc.find('table', class_='tableFile2')\n",
    "\n",
    "            ten_k_entries_rows = list_of_all_ten_k_entries_table.find_all('tr')  # get the first row because it has the most recent 10K\n",
    "\n",
    "            most_recent_ten_k_link = None\n",
    "\n",
    "            for one_ten_k_entries_row in ten_k_entries_rows:\n",
    "                one_ten_k_entries_row_cells = one_ten_k_entries_row.find_all('td')  # Assuming each cell is represented by a <td> tag\n",
    "            #     print(one_ten_k_entries_row_cells)\n",
    "\n",
    "                for one_ten_k_entries_row_cell in one_ten_k_entries_row_cells:\n",
    "            #         print(one_ten_k_entries_row_cell)\n",
    "                    if one_ten_k_entries_row_cell.text.strip() == '10-K':\n",
    "                        ten_k_document_link_href = one_ten_k_entries_row_cell.find_next('td').find('a')['href']\n",
    "\n",
    "                        ten_k_document_link_base = \"https://www.sec.gov/\"\n",
    "                        ten_k_document_link_tail = ten_k_document_link_href\n",
    "                        ten_k_document_link = ten_k_document_link_base + ten_k_document_link_tail\n",
    "\n",
    "            #             print(document_link)\n",
    "\n",
    "                        # Store the first 10-K link and break out of the loop\n",
    "                        most_recent_ten_k_link = ten_k_document_link\n",
    "                        break\n",
    "\n",
    "                if most_recent_ten_k_link is not None:\n",
    "                    break\n",
    "\n",
    "\n",
    "            # ONCE I HAVE THE TEN K DOC LINK \n",
    "            # 3. Now on the page with the document and graphics table\n",
    "\n",
    "            ten_k_doc_link_playwright = await async_playwright().start()\n",
    "            ten_k_doc_link_browser = await ten_k_doc_link_playwright.chromium.launch(headless = False)\n",
    "            ten_k_doc_link_page = await ten_k_doc_link_browser.new_page()\n",
    "\n",
    "            time.sleep(6)\n",
    "\n",
    "            await ten_k_doc_link_page.goto(most_recent_ten_k_link)\n",
    "            time.sleep(6)\n",
    "\n",
    "\n",
    "\n",
    "            # Initiate the html and beautiful soup content \n",
    "\n",
    "            list_of_doc_and_graphics_response= await ten_k_doc_link_page.content()\n",
    "\n",
    "            list_of_doc_and_graphics_soup_doc = BeautifulSoup(list_of_doc_and_graphics_response, 'html.parser')\n",
    "\n",
    "            # get actual_full_ten_k_document_link\n",
    "            list_of_doc_and_graphics_table = list_of_doc_and_graphics_soup_doc.find('table', class_='tableFile')\n",
    "\n",
    "\n",
    "            list_of_doc_and_graphics_rows = list_of_doc_and_graphics_table.find_all('tr')  # Assuming each row is represented by a <tr> tag\n",
    "\n",
    "            for list_of_doc_and_graphics_row in list_of_doc_and_graphics_rows:\n",
    "                list_of_doc_and_graphics_cells = list_of_doc_and_graphics_row.find_all('td')  # Assuming each cell is represented by a <td> tag\n",
    "            #     print(cells)\n",
    "                for list_of_doc_and_graphics_cell in list_of_doc_and_graphics_cells:\n",
    "                    if list_of_doc_and_graphics_cell.text.strip() == '10-K':\n",
    "                        actual_document_link = list_of_doc_and_graphics_cell.find_next('td').find('a')\n",
    "                        if actual_document_link is not None:\n",
    "                            actual_ten_k_document_href = actual_document_link['href']\n",
    "            #                 print(\"Documents href:\", document_href)\n",
    "                            actual_ten_k_document_link_base=\"https://www.sec.gov/\"\n",
    "                            actual_ten_k_document_link_tail=actual_ten_k_document_href\n",
    "                            actual_full_ten_k_document_link=actual_ten_k_document_link_base+actual_ten_k_document_link_tail\n",
    "                            actual_full_ten_k_document_link=x\n",
    "                            print(actual_full_ten_k_document_link)\n",
    "                            print(\"#########\")\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        finally:\n",
    "            # Close the page and browser\n",
    "            await ten_k_doc_link_page.close()\n",
    "            await ten_k_doc_link_browser.close()\n",
    "            await ten_k_doc_link_playwright.stop()\n",
    "\n",
    "            # Close the sec_home page and browser\n",
    "            await sec_home_page.close()\n",
    "            await sec_home_browser.close()\n",
    "            await sec_home_playwright.stop()\n",
    "\n",
    "    # with change for link\n",
    "\n",
    "\n",
    "    list_of_actual_full_ten_k_document_links = [\"https://www.sec.gov/ix?doc=/Archives/edgar/data/789019/000156459022026876/msft-10k_20220630.htm\"]\n",
    "\n",
    "    actual_list_of_actual_full_ten_k_document_links = [\"https://www.sec.gov//ix?doc=/Archives/edgar/data/320193/000032019322000108/aapl-20220924.htm\",\"https://www.sec.gov/ix?doc=/Archives/edgar/data/789019/000156459022026876/msft-10k_20220630.htm\",\"https://www.sec.gov/ix?doc=/Archives/edgar/data/1652044/000165204423000016/goog-20221231.htm\"]\n",
    "\n",
    "    async def get_actual_full_ten_k_document_content(x):\n",
    "        playwright = await async_playwright().start()\n",
    "        browser = await playwright.chromium.launch(headless=False)\n",
    "    #     browser = await playwright.firefox.launch(headless=False)\n",
    "\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        await page.goto(x)\n",
    "        time.sleep(10)\n",
    "\n",
    "        while True:\n",
    "            # Get the number of spans on the page\n",
    "            spans_assigned = await page.query_selector_all(\"span\")\n",
    "            test = len(spans_assigned)\n",
    "\n",
    "            # Scroll in increments of 5000 pixels\n",
    "            await page.evaluate('''() => {\n",
    "                let distance = 5000;\n",
    "                window.scrollBy(0, distance);\n",
    "            }''')\n",
    "    #         await asyncio.sleep(3)\n",
    "\n",
    "            time.sleep(10)\n",
    "\n",
    "\n",
    "            spans_assigned_two = await page.query_selector_all(\"span\")\n",
    "            test2 = len(spans_assigned_two)\n",
    "            if test == test2:\n",
    "                break\n",
    "\n",
    "        # Get the content of the page\n",
    "        actual_full_ten_k_document_content = await page.content()\n",
    "        time.sleep(10)\n",
    "\n",
    "        await browser.close()\n",
    "        time.sleep(10)\n",
    "        await page.close()  # Add this line to close the page\n",
    "        time.sleep(10)\n",
    "\n",
    "        return actual_full_ten_k_document_content\n",
    "\n",
    "\n",
    "    async def process_actual_full_ten_k_document(actual_full_ten_k_document_content):\n",
    "        actual_full_ten_k_document_soup = BeautifulSoup(actual_full_ten_k_document_content, 'html.parser')\n",
    "        return actual_full_ten_k_document_soup\n",
    "\n",
    "\n",
    "    async def main():\n",
    "        dict_titles=['actual_full_ten_k_document_link_collection',\n",
    "                     'table_length',\n",
    "                     'list_of_stock_based_values',\n",
    "                     'first_stock_based_compensation',\n",
    "                     'second_stock_based_compensation',\n",
    "                     'third_stock_based_compensation']\n",
    "\n",
    "        possible_titles = ['CONSOLIDATED STATEMENTS OF CASH FLOWS',\n",
    "                           'CASH FLOWS STATEMENTS',\n",
    "                           'Consolidated Statements of Cash Flows',\n",
    "                           'Consolidated Statement of Cash Flows',\n",
    "                          'CONSOLIDATED STATEMENT OF CASH FLOWS',\n",
    "                          'STATEMENTS OF CONSOLIDATED CASH FLOWS',\n",
    "                          'Consolidated Cash Flow Statement',\n",
    "                          'Consolidated Statement of Cash Flow for the Years Ended December 31',\n",
    "                          'STATEMENT OF CASH FLOWS',\n",
    "                          'CONSOLIDATED STATEMENTS OF CASH FLOW',\n",
    "                          'Statements of Consolidated Cash Flows']\n",
    "\n",
    "        stock_related_texts = [\n",
    "            'Stock-based compensation expense',\n",
    "            'Stock-based compensation expense',\n",
    "            'Share-based compensation expense',\n",
    "            'Stock-based compensation',\n",
    "            'Share-based compensation'\n",
    "        ]\n",
    "\n",
    "        all_ten_k_document_lists = []\n",
    "\n",
    "\n",
    "    #     list_of_example_actual_full_ten_k_document_soup_doc = []\n",
    "\n",
    "        # Define the group size\n",
    "        group_size = 2\n",
    "        items=df_stock_top_us_plus_sec_titles['actual_full_ten_k_document_link']\n",
    "\n",
    "        # Iterate over groups of items\n",
    "        for start_index in range(0, len(items), group_size):\n",
    "            group_items = items[start_index:start_index + group_size]\n",
    "\n",
    "            list_of_example_actual_full_ten_k_document_soup_doc = []\n",
    "\n",
    "\n",
    "    #         # Iterate over items within the group\n",
    "    #         for item in group_items:\n",
    "    #             print(item)\n",
    "\n",
    "    #         # Add a separator for each group\n",
    "    #         print(\"Group separator\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            url_to_parsed_content = {}\n",
    "\n",
    "\n",
    "            for actual_full_ten_k_document_link in group_items:\n",
    "                try:\n",
    "                    actual_full_ten_k_document_link_collection = actual_full_ten_k_document_link\n",
    "                    print(actual_full_ten_k_document_link)\n",
    "\n",
    "                    for _ in range(3):\n",
    "                        try:\n",
    "                            actual_full_ten_k_document_content = await get_actual_full_ten_k_document_content(actual_full_ten_k_document_link)\n",
    "                            example_actual_full_ten_k_document_soup_doc = await process_actual_full_ten_k_document(actual_full_ten_k_document_content)\n",
    "                            list_of_example_actual_full_ten_k_document_soup_doc.append(example_actual_full_ten_k_document_soup_doc)\n",
    "                                # Store the parsed content in the dictionary.\n",
    "                            url_to_parsed_content[actual_full_ten_k_document_link] = example_actual_full_ten_k_document_soup_doc\n",
    "\n",
    "                            break  # Break the loop if successful\n",
    "                        except Exception as e:\n",
    "                            print(f\"An error occurred: {str(e)}. Retrying...\")\n",
    "                            time.sleep(15)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            for actual_full_ten_k_document_link, one_actual_full_ten_k_document_soup_doc in url_to_parsed_content.items():\n",
    "                ten_k_detail_list=[]\n",
    "\n",
    "                if one_actual_full_ten_k_document_soup_doc:\n",
    "\n",
    "                    elements = one_actual_full_ten_k_document_soup_doc.find_all(text=possible_titles)\n",
    "\n",
    "                    if not elements:\n",
    "                        print(\"Titles not found in the HTML.\")\n",
    "                        continue\n",
    "\n",
    "                    for element in elements:\n",
    "                        if element.strip() in possible_titles:\n",
    "                            following_elements = element.find_all_next()\n",
    "                            table = None\n",
    "\n",
    "                            # Iterate through following elements and stop when a table is found or after three components\n",
    "                            component_count = 0\n",
    "                            for next_element in following_elements:\n",
    "                                if next_element.name == 'table' and any(text in next_element.get_text() for text in stock_related_texts):\n",
    "                                    table = next_element\n",
    "                                    break\n",
    "                                elif next_element.name in ['p', 'span']:\n",
    "                                    component_count += 1\n",
    "                                    if component_count >= 5:\n",
    "                                        break\n",
    "\n",
    "                            if table is None:\n",
    "                                tables = one_actual_full_ten_k_document_soup_doc.find_all(\"table\")\n",
    "                                for t in tables:     \n",
    "                                    if t:\n",
    "                                        table_text = t.get_text()\n",
    "                                        if any(title in table_text for title in possible_titles) and any(stock_related_text in table_text for stock_related_text in stock_related_texts):\n",
    "                                            rows = t.find_all('tr')\n",
    "                                            table_length = len(rows)\n",
    "                                            print(f\"Table Length from within: {len(rows)}\") \n",
    "                                        break\n",
    "\n",
    "                            if table:\n",
    "                                rows = table.find_all('tr')\n",
    "                                table_length=len(rows)\n",
    "\n",
    "                                print(f\"Table Length: {len(rows)}\")                        \n",
    "\n",
    "                                break\n",
    "\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        print(\"No table found within or after the possible titles in the HTML.\")\n",
    "\n",
    "                else:\n",
    "                    print(\"Document not processed, skipping\")\n",
    "\n",
    "\n",
    "                if table:\n",
    "                    target_td = table.find('td', text=stock_related_texts)\n",
    "                else:\n",
    "                    target_td = None\n",
    "\n",
    "                if target_td:\n",
    "                    # Move up to the <tr> containing the <td> with \"share-based compensation\"\n",
    "                    target_tr = target_td.find_parent('tr')\n",
    "\n",
    "                    # Extract all the <td> values from the <tr> and put them in a column\n",
    "                    values_in_column = [td.get_text(strip=True) for td in target_tr.find_all('td')]\n",
    "\n",
    "                    # Output the values in the column\n",
    "                    if values_in_column:\n",
    "                        stock_stock_based_compensation_title=values_in_column[0]\n",
    "                        list_of_stock_based_values=values_in_column\n",
    "                        numbers_only_values_in_column = [entry for entry in values_in_column if any(char.isdigit() for char in entry)]\n",
    "                        first_stock_based_compensation=numbers_only_values_in_column[0]\n",
    "                        second_stock_based_compensation=numbers_only_values_in_column[1]\n",
    "                        third_stock_based_compensation=numbers_only_values_in_column[2]\n",
    "                        for value in values_in_column:\n",
    "                            print(value)\n",
    "\n",
    "                    ten_k_detail_list.append(actual_full_ten_k_document_link)\n",
    "                    ten_k_detail_list.append(table_length)\n",
    "                    ten_k_detail_list.append(list_of_stock_based_values)\n",
    "                    ten_k_detail_list.append(first_stock_based_compensation)\n",
    "                    ten_k_detail_list.append(second_stock_based_compensation)\n",
    "                    ten_k_detail_list.append(third_stock_based_compensation)\n",
    "\n",
    "                    first_rep = dict(zip(dict_titles, ten_k_detail_list))\n",
    "                    all_ten_k_document_lists.append(first_rep)\n",
    "\n",
    "\n",
    "\n",
    "                else:\n",
    "                    print(\"The 'Share-Based Compensation' row not found!\")\n",
    "                time.sleep(10)\n",
    "\n",
    "    #     print(all_ten_k_document_lists)\n",
    "        return all_ten_k_document_lists\n",
    "\n",
    "\n",
    "    # Run the main coroutine\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    # Use asyncio.run() to run the main coroutine\n",
    "    all_ten_k_document_lists_of_dicts=asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22db55d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ae5825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47992932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0cf873",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe78f38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48990ec5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf83f18f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c57b1fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "await extracting_stock_compensation_for_one_company(one_title_searchable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcae2d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c36cb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580cb461",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde20ba5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e89c88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b94abd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29bf6508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97645265",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
