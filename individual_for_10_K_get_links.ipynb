{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5015f591",
   "metadata": {},
   "source": [
    "# Setting Everything Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3283eb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0388ff33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the file from the web\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from unicodedata import normalize\n",
    "import re\n",
    "from datetime import datetime\n",
    "from pandas import read_csv \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import json\n",
    "from flatten_json import flatten\n",
    "\n",
    "\n",
    "# I can give a number or use None to remove maximum ceiling & display all columns\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# # I want to be able to see the entire narrative, so remove the maximum width for each column\n",
    "# pd.options.display.max_colwidth = None\n",
    "\n",
    "# pd.options.display.float_format = '{:,.0f}'.format\n",
    "\n",
    "import string\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42b42014",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline  \n",
    "from matplotlib import rcParams\n",
    "rcParams['figure.figsize'] = (8, 8)\n",
    "\n",
    "import warnings\n",
    "from rpy2.rinterface import RRuntimeWarning\n",
    "warnings.filterwarnings(\"ignore\") # Ignore all warnings\n",
    "# warnings.filterwarnings(\"ignore\", category=RRuntimeWarning) # Show some warnings\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "242498c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "// Disable auto-scrolling\n",
       "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
       "    return false;\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "// Disable auto-scrolling\n",
    "IPython.OutputArea.prototype._should_scroll = function(lines) {\n",
    "    return false;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "739fccf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Loading required package: tidyverse\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n",
      "✔ dplyr     1.1.2     ✔ readr     2.1.4\n",
      "✔ forcats   1.0.0     ✔ stringr   1.5.0\n",
      "✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n",
      "✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n",
      "✔ purrr     1.0.1     \n",
      "── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "✖ dplyr::filter() masks stats::filter()\n",
      "✖ dplyr::lag()    masks stats::lag()\n",
      "ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "# My commonly used R imports\n",
    "\n",
    "require('tidyverse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4aa539ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To Download PDFs\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e57e778",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To import camelot and PDF-related items\n",
    "import camelot\n",
    "import ghostscript\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "079713d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Loading required package: RColorBrewer\n",
      "\n",
      "R[write to console]: Loading required package: NLP\n",
      "\n",
      "R[write to console]: \n",
      "Attaching package: ‘NLP’\n",
      "\n",
      "\n",
      "R[write to console]: The following object is masked from ‘package:ggplot2’:\n",
      "\n",
      "    annotate\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "\n",
    "library(jpeg)\n",
    "library(wordcloud)\n",
    "library(RColorBrewer)\n",
    "library(wordcloud2)\n",
    "library(tm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d016e49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6da3362c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: \n",
      "Attaching package: ‘plotly’\n",
      "\n",
      "\n",
      "R[write to console]: The following object is masked from ‘package:ggplot2’:\n",
      "\n",
      "    last_plot\n",
      "\n",
      "\n",
      "R[write to console]: The following object is masked from ‘package:stats’:\n",
      "\n",
      "    filter\n",
      "\n",
      "\n",
      "R[write to console]: The following object is masked from ‘package:graphics’:\n",
      "\n",
      "    layout\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "library(vistime)\n",
    "library(shiny)\n",
    "library(plotly)\n",
    "#for timeline plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6204946d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecbb534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for playwright stuff\n",
    "import time\n",
    "from playwright.async_api import async_playwright\n",
    "import asyncio\n",
    "import nest_asyncio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5350a494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install flask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45d3883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1675ca1f",
   "metadata": {},
   "source": [
    " # Getting 10K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a29138e",
   "metadata": {},
   "source": [
    "# Putting together code for finding the 10k link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55b0392",
   "metadata": {},
   "source": [
    "#### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d61a04f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # whatever title is input\n",
    "# one_title_searchable=\"Apple Inc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bc1ff00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is final\n",
    "\n",
    "\n",
    "async def extracting_stock_compensation_for_one_company():\n",
    "    \n",
    "\n",
    "    async def getting_10_k_link(one_title_searchable):\n",
    "        #### ON THE SEC HOME PAGE\n",
    "        # going to sec page\n",
    "        sec_home_playwright = await async_playwright().start()\n",
    "        sec_home_browser = await sec_home_playwright.chromium.launch(headless = False)\n",
    "        sec_home_page = await sec_home_browser.new_page()\n",
    "\n",
    "        await sec_home_page.goto(\"https://www.sec.gov/edgar/searchedgar/companysearch\")\n",
    "        time.sleep(6)\n",
    "\n",
    "        try:\n",
    "\n",
    "            # input company name (here is where we can enter title variable from df from csv)\n",
    "\n",
    "            await sec_home_page.fill(\"xpath=/html/body/div[1]/div/div/div/div[3]/div/div[2]/div[2]/div/div/article/div/div/div[1]/div[2]/div/div/div/div[1]/form/div[2]/input\", one_title_searchable)\n",
    "            time.sleep(6)\n",
    "\n",
    "            # click search\n",
    "\n",
    "            await sec_home_page.click(\"xpath=/html/body/div[1]/div/div/div/div[3]/div/div[2]/div[2]/div/div/article/div/div/div[1]/div[2]/div/div/div/div[1]/form/div[2]/button\")\n",
    "            time.sleep(6)\n",
    "\n",
    "            #============\n",
    "            # ON THE NEW PAGE RESULTING WITH ALL COMPANY DOCS\n",
    "            #input 10K for search\n",
    "            await sec_home_page.fill(\"xpath=/html/body/div[4]/div[2]/form/table/tbody/tr[3]/td[1]/input\", \"10-K\")\n",
    "            time.sleep(6)\n",
    "\n",
    "            # click search\n",
    "\n",
    "            await sec_home_page.click(\"xpath=/html/body/div[4]/div[2]/form/table/tbody/tr[3]/td[5]/input[1]\")\n",
    "            time.sleep(6)\n",
    "\n",
    "\n",
    "            #get all 10K entries\n",
    "            list_of_all_ten_k_entries_response= await sec_home_page.content()\n",
    "\n",
    "            list_of_all_ten_k_entries_doc = BeautifulSoup(list_of_all_ten_k_entries_response, 'html.parser')\n",
    "\n",
    "            #get the first 10K\n",
    "            list_of_all_ten_k_entries_table = list_of_all_ten_k_entries_doc.find('table', class_='tableFile2')\n",
    "\n",
    "            ten_k_entries_rows = list_of_all_ten_k_entries_table.find_all('tr')  # get the first row because it has the most recent 10K\n",
    "\n",
    "            most_recent_ten_k_link = None\n",
    "\n",
    "            for one_ten_k_entries_row in ten_k_entries_rows:\n",
    "                one_ten_k_entries_row_cells = one_ten_k_entries_row.find_all('td')  # Assuming each cell is represented by a <td> tag\n",
    "            #     print(one_ten_k_entries_row_cells)\n",
    "\n",
    "                for one_ten_k_entries_row_cell in one_ten_k_entries_row_cells:\n",
    "            #         print(one_ten_k_entries_row_cell)\n",
    "                    if one_ten_k_entries_row_cell.text.strip() == '10-K':\n",
    "                        ten_k_document_link_href = one_ten_k_entries_row_cell.find_next('td').find('a')['href']\n",
    "\n",
    "                        ten_k_document_link_base = \"https://www.sec.gov/\"\n",
    "                        ten_k_document_link_tail = ten_k_document_link_href\n",
    "                        ten_k_document_link = ten_k_document_link_base + ten_k_document_link_tail\n",
    "\n",
    "            #             print(document_link)\n",
    "\n",
    "                        # Store the first 10-K link and break out of the loop\n",
    "                        most_recent_ten_k_link = ten_k_document_link\n",
    "                        break\n",
    "\n",
    "                if most_recent_ten_k_link is not None:\n",
    "                    break\n",
    "\n",
    "\n",
    "            # ONCE I HAVE THE TEN K DOC LINK \n",
    "            # 3. Now on the page with the document and graphics table\n",
    "\n",
    "            ten_k_doc_link_playwright = await async_playwright().start()\n",
    "            ten_k_doc_link_browser = await ten_k_doc_link_playwright.chromium.launch(headless = False)\n",
    "            ten_k_doc_link_page = await ten_k_doc_link_browser.new_page()\n",
    "\n",
    "            time.sleep(6)\n",
    "\n",
    "            await ten_k_doc_link_page.goto(most_recent_ten_k_link)\n",
    "            time.sleep(6)\n",
    "\n",
    "\n",
    "\n",
    "            # Initiate the html and beautiful soup content \n",
    "\n",
    "            list_of_doc_and_graphics_response= await ten_k_doc_link_page.content()\n",
    "\n",
    "            list_of_doc_and_graphics_soup_doc = BeautifulSoup(list_of_doc_and_graphics_response, 'html.parser')\n",
    "\n",
    "            # get actual_full_ten_k_document_link\n",
    "            list_of_doc_and_graphics_table = list_of_doc_and_graphics_soup_doc.find('table', class_='tableFile')\n",
    "\n",
    "\n",
    "            list_of_doc_and_graphics_rows = list_of_doc_and_graphics_table.find_all('tr')  # Assuming each row is represented by a <tr> tag\n",
    "\n",
    "            for list_of_doc_and_graphics_row in list_of_doc_and_graphics_rows:\n",
    "                list_of_doc_and_graphics_cells = list_of_doc_and_graphics_row.find_all('td')  # Assuming each cell is represented by a <td> tag\n",
    "            #     print(cells)\n",
    "                for list_of_doc_and_graphics_cell in list_of_doc_and_graphics_cells:\n",
    "                    if list_of_doc_and_graphics_cell.text.strip() == '10-K':\n",
    "                        actual_document_link = list_of_doc_and_graphics_cell.find_next('td').find('a')\n",
    "                        if actual_document_link is not None:\n",
    "                            actual_ten_k_document_href = actual_document_link['href']\n",
    "            #                 print(\"Documents href:\", document_href)\n",
    "                            actual_ten_k_document_link_base=\"https://www.sec.gov/\"\n",
    "                            actual_ten_k_document_link_tail=actual_ten_k_document_href\n",
    "                            actual_full_ten_k_document_link=actual_ten_k_document_link_base+actual_ten_k_document_link_tail\n",
    "#                             global x\n",
    "                            x=actual_full_ten_k_document_link\n",
    "                            print(actual_full_ten_k_document_link)\n",
    "                            print(\"#########\")\n",
    "\n",
    "                            \n",
    "\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        finally:\n",
    "            # Close the page and browser\n",
    "            await ten_k_doc_link_page.close()\n",
    "            await ten_k_doc_link_browser.close()\n",
    "            await ten_k_doc_link_playwright.stop()\n",
    "\n",
    "            # Close the sec_home page and browser\n",
    "            await sec_home_page.close()\n",
    "            await sec_home_browser.close()\n",
    "            await sec_home_playwright.stop()\n",
    "            return actual_full_ten_k_document_link\n",
    "    \n",
    "    \n",
    "    \n",
    "    x = await getting_10_k_link(one_title_searchable)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # with change for link\n",
    "\n",
    "\n",
    "    list_of_actual_full_ten_k_document_links = [\"https://www.sec.gov/ix?doc=/Archives/edgar/data/789019/000156459022026876/msft-10k_20220630.htm\"]\n",
    "\n",
    "    actual_list_of_actual_full_ten_k_document_links = [\"https://www.sec.gov//ix?doc=/Archives/edgar/data/320193/000032019322000108/aapl-20220924.htm\",\"https://www.sec.gov/ix?doc=/Archives/edgar/data/789019/000156459022026876/msft-10k_20220630.htm\",\"https://www.sec.gov/ix?doc=/Archives/edgar/data/1652044/000165204423000016/goog-20221231.htm\"]\n",
    "\n",
    "    async def get_actual_full_ten_k_document_content(x):\n",
    "        playwright = await async_playwright().start()\n",
    "        browser = await playwright.chromium.launch(headless=False)\n",
    "    #     browser = await playwright.firefox.launch(headless=False)\n",
    "\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        await page.goto(x)\n",
    "        time.sleep(10)\n",
    "\n",
    "        while True:\n",
    "            # Get the number of spans on the page\n",
    "            spans_assigned = await page.query_selector_all(\"span\")\n",
    "            test = len(spans_assigned)\n",
    "\n",
    "            # Scroll in increments of 5000 pixels\n",
    "            await page.evaluate('''() => {\n",
    "                let distance = 5000;\n",
    "                window.scrollBy(0, distance);\n",
    "            }''')\n",
    "    #         await asyncio.sleep(3)\n",
    "\n",
    "            time.sleep(10)\n",
    "\n",
    "\n",
    "            spans_assigned_two = await page.query_selector_all(\"span\")\n",
    "            test2 = len(spans_assigned_two)\n",
    "            if test == test2:\n",
    "                break\n",
    "\n",
    "        # Get the content of the page\n",
    "        actual_full_ten_k_document_content = await page.content()\n",
    "        time.sleep(10)\n",
    "\n",
    "        await browser.close()\n",
    "        time.sleep(10)\n",
    "        await page.close()  # Add this line to close the page\n",
    "        time.sleep(10)\n",
    "\n",
    "        return actual_full_ten_k_document_content\n",
    "\n",
    "\n",
    "    async def process_actual_full_ten_k_document(actual_full_ten_k_document_content):\n",
    "        actual_full_ten_k_document_soup = BeautifulSoup(actual_full_ten_k_document_content, 'html.parser')\n",
    "        return actual_full_ten_k_document_soup\n",
    "\n",
    "\n",
    "    async def main():\n",
    "        dict_titles=['actual_full_ten_k_document_link_collection',\n",
    "                     'table_length',\n",
    "                     'list_of_stock_based_values',\n",
    "                     'first_stock_based_compensation',\n",
    "                     'second_stock_based_compensation',\n",
    "                     'third_stock_based_compensation']\n",
    "\n",
    "        possible_titles = ['CONSOLIDATED STATEMENTS OF CASH FLOWS',\n",
    "                           'CASH FLOWS STATEMENTS',\n",
    "                           'Consolidated Statements of Cash Flows',\n",
    "                           'Consolidated Statement of Cash Flows',\n",
    "                          'CONSOLIDATED STATEMENT OF CASH FLOWS',\n",
    "                          'STATEMENTS OF CONSOLIDATED CASH FLOWS',\n",
    "                          'Consolidated Cash Flow Statement',\n",
    "                          'Consolidated Statement of Cash Flow for the Years Ended December 31',\n",
    "                          'STATEMENT OF CASH FLOWS',\n",
    "                          'CONSOLIDATED STATEMENTS OF CASH FLOW',\n",
    "                          'Statements of Consolidated Cash Flows']\n",
    "\n",
    "        stock_related_texts = [\n",
    "            'Stock-based compensation expense',\n",
    "            'Stock-based compensation expense',\n",
    "            'Share-based compensation expense',\n",
    "            'Stock-based compensation',\n",
    "            'Share-based compensation'\n",
    "        ]\n",
    "\n",
    "        all_ten_k_document_lists = []\n",
    "\n",
    "\n",
    "    #     list_of_example_actual_full_ten_k_document_soup_doc = []\n",
    "\n",
    "        # Define the group size\n",
    "        group_size = 2\n",
    "        items=[x]\n",
    "\n",
    "        # Iterate over groups of items\n",
    "        for start_index in range(0, len(items), group_size):\n",
    "            group_items = items[start_index:start_index + group_size]\n",
    "\n",
    "            list_of_example_actual_full_ten_k_document_soup_doc = []\n",
    "\n",
    "\n",
    "    #         # Iterate over items within the group\n",
    "    #         for item in group_items:\n",
    "    #             print(item)\n",
    "\n",
    "    #         # Add a separator for each group\n",
    "    #         print(\"Group separator\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            url_to_parsed_content = {}\n",
    "\n",
    "\n",
    "            for actual_full_ten_k_document_link in group_items:\n",
    "                try:\n",
    "                    actual_full_ten_k_document_link_collection = actual_full_ten_k_document_link\n",
    "                    print(actual_full_ten_k_document_link)\n",
    "\n",
    "                    for _ in range(3):\n",
    "                        try:\n",
    "                            actual_full_ten_k_document_content = await get_actual_full_ten_k_document_content(actual_full_ten_k_document_link)\n",
    "                            example_actual_full_ten_k_document_soup_doc = await process_actual_full_ten_k_document(actual_full_ten_k_document_content)\n",
    "                            list_of_example_actual_full_ten_k_document_soup_doc.append(example_actual_full_ten_k_document_soup_doc)\n",
    "                                # Store the parsed content in the dictionary.\n",
    "                            url_to_parsed_content[actual_full_ten_k_document_link] = example_actual_full_ten_k_document_soup_doc\n",
    "\n",
    "                            break  # Break the loop if successful\n",
    "                        except Exception as e:\n",
    "                            print(f\"An error occurred: {str(e)}. Retrying...\")\n",
    "                            time.sleep(15)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            for actual_full_ten_k_document_link, one_actual_full_ten_k_document_soup_doc in url_to_parsed_content.items():\n",
    "                ten_k_detail_list=[]\n",
    "\n",
    "                if one_actual_full_ten_k_document_soup_doc:\n",
    "\n",
    "                    elements = one_actual_full_ten_k_document_soup_doc.find_all(text=possible_titles)\n",
    "\n",
    "                    if not elements:\n",
    "                        print(\"Titles not found in the HTML.\")\n",
    "                        continue\n",
    "\n",
    "                    for element in elements:\n",
    "                        if element.strip() in possible_titles:\n",
    "                            following_elements = element.find_all_next()\n",
    "                            table = None\n",
    "\n",
    "                            # Iterate through following elements and stop when a table is found or after three components\n",
    "                            component_count = 0\n",
    "                            for next_element in following_elements:\n",
    "                                if next_element.name == 'table' and any(text in next_element.get_text() for text in stock_related_texts):\n",
    "                                    table = next_element\n",
    "                                    break\n",
    "                                elif next_element.name in ['p', 'span']:\n",
    "                                    component_count += 1\n",
    "                                    if component_count >= 5:\n",
    "                                        break\n",
    "\n",
    "                            if table is None:\n",
    "                                tables = one_actual_full_ten_k_document_soup_doc.find_all(\"table\")\n",
    "                                for t in tables:     \n",
    "                                    if t:\n",
    "                                        table_text = t.get_text()\n",
    "                                        if any(title in table_text for title in possible_titles) and any(stock_related_text in table_text for stock_related_text in stock_related_texts):\n",
    "                                            rows = t.find_all('tr')\n",
    "                                            table_length = len(rows)\n",
    "                                            print(f\"Table Length from within: {len(rows)}\") \n",
    "                                        break\n",
    "\n",
    "                            if table:\n",
    "                                rows = table.find_all('tr')\n",
    "                                table_length=len(rows)\n",
    "\n",
    "                                print(f\"Table Length: {len(rows)}\")                        \n",
    "\n",
    "                                break\n",
    "\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        print(\"No table found within or after the possible titles in the HTML.\")\n",
    "\n",
    "                else:\n",
    "                    print(\"Document not processed, skipping\")\n",
    "\n",
    "\n",
    "                if table:\n",
    "                    target_td = table.find('td', text=stock_related_texts)\n",
    "                else:\n",
    "                    target_td = None\n",
    "\n",
    "                if target_td:\n",
    "                    # Move up to the <tr> containing the <td> with \"share-based compensation\"\n",
    "                    target_tr = target_td.find_parent('tr')\n",
    "\n",
    "                    # Extract all the <td> values from the <tr> and put them in a column\n",
    "                    values_in_column = [td.get_text(strip=True) for td in target_tr.find_all('td')]\n",
    "\n",
    "                    # Output the values in the column\n",
    "                    if values_in_column:\n",
    "                        stock_stock_based_compensation_title=values_in_column[0]\n",
    "                        list_of_stock_based_values=values_in_column\n",
    "                        numbers_only_values_in_column = [entry for entry in values_in_column if any(char.isdigit() for char in entry)]\n",
    "                        first_stock_based_compensation=numbers_only_values_in_column[0]\n",
    "                        second_stock_based_compensation=numbers_only_values_in_column[1]\n",
    "                        third_stock_based_compensation=numbers_only_values_in_column[2]\n",
    "                        for value in values_in_column:\n",
    "                            print(value)\n",
    "\n",
    "                    ten_k_detail_list.append(actual_full_ten_k_document_link)\n",
    "                    ten_k_detail_list.append(table_length)\n",
    "                    ten_k_detail_list.append(list_of_stock_based_values)\n",
    "                    ten_k_detail_list.append(first_stock_based_compensation)\n",
    "                    ten_k_detail_list.append(second_stock_based_compensation)\n",
    "                    ten_k_detail_list.append(third_stock_based_compensation)\n",
    "\n",
    "                    first_rep = dict(zip(dict_titles, ten_k_detail_list))\n",
    "                    all_ten_k_document_lists.append(first_rep)\n",
    "\n",
    "\n",
    "\n",
    "                else:\n",
    "                    print(\"The 'Share-Based Compensation' row not found!\")\n",
    "                time.sleep(10)\n",
    "\n",
    "    #     print(all_ten_k_document_lists)\n",
    "        return all_ten_k_document_lists\n",
    "\n",
    "\n",
    "    # Run the main coroutine\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    # Use asyncio.run() to run the main coroutine\n",
    "    all_ten_k_document_lists_of_dicts=asyncio.run(main())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f872eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !brew tap heroku/brew && brew install heroku\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ebf3711a",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'ten_k_doc_link_page' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mawait\u001b[39;00m extracting_stock_compensation_for_one_company()\n",
      "Cell \u001b[0;32mIn [17], line 139\u001b[0m, in \u001b[0;36mextracting_stock_compensation_for_one_company\u001b[0;34m()\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[38;5;28;01mawait\u001b[39;00m sec_home_playwright\u001b[38;5;241m.\u001b[39mstop()\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m actual_full_ten_k_document_link\n\u001b[0;32m--> 139\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m getting_10_k_link(one_title_searchable)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# with change for link\u001b[39;00m\n\u001b[1;32m    149\u001b[0m list_of_actual_full_ten_k_document_links \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://www.sec.gov/ix?doc=/Archives/edgar/data/789019/000156459022026876/msft-10k_20220630.htm\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn [17], line 127\u001b[0m, in \u001b[0;36mextracting_stock_compensation_for_one_company.<locals>.getting_10_k_link\u001b[0;34m(one_title_searchable)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;66;03m# Close the page and browser\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[43mten_k_doc_link_page\u001b[49m\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m ten_k_doc_link_browser\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m ten_k_doc_link_playwright\u001b[38;5;241m.\u001b[39mstop()\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'ten_k_doc_link_page' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "await extracting_stock_compensation_for_one_company()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf91115f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a82dd87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebaf4fe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98089e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# whatever title is input\n",
    "one_title_searchable=\"Apple Inc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80816a5a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# this is final\n",
    "\n",
    "async def getting_10_k_link(one_title_searchable):\n",
    "    #### ON THE SEC HOME PAGE\n",
    "    # going to sec page\n",
    "    sec_home_playwright = await async_playwright().start()\n",
    "    sec_home_browser = await sec_home_playwright.chromium.launch(headless = False)\n",
    "    sec_home_page = await sec_home_browser.new_page()\n",
    "\n",
    "    await sec_home_page.goto(\"https://www.sec.gov/edgar/searchedgar/companysearch\")\n",
    "    time.sleep(6)\n",
    "\n",
    "    try:\n",
    "\n",
    "        # input company name (here is where we can enter title variable from df from csv)\n",
    "\n",
    "        await sec_home_page.fill(\"xpath=/html/body/div[1]/div/div/div/div[3]/div/div[2]/div[2]/div/div/article/div/div/div[1]/div[2]/div/div/div/div[1]/form/div[2]/input\", one_title_searchable)\n",
    "        time.sleep(6)\n",
    "\n",
    "        # click search\n",
    "\n",
    "        await sec_home_page.click(\"xpath=/html/body/div[1]/div/div/div/div[3]/div/div[2]/div[2]/div/div/article/div/div/div[1]/div[2]/div/div/div/div[1]/form/div[2]/button\")\n",
    "        time.sleep(6)\n",
    "\n",
    "        #============\n",
    "        # ON THE NEW PAGE RESULTING WITH ALL COMPANY DOCS\n",
    "        #input 10K for search\n",
    "        await sec_home_page.fill(\"xpath=/html/body/div[4]/div[2]/form/table/tbody/tr[3]/td[1]/input\", \"10-K\")\n",
    "        time.sleep(6)\n",
    "\n",
    "        # click search\n",
    "\n",
    "        await sec_home_page.click(\"xpath=/html/body/div[4]/div[2]/form/table/tbody/tr[3]/td[5]/input[1]\")\n",
    "        time.sleep(6)\n",
    "\n",
    "\n",
    "        #get all 10K entries\n",
    "        list_of_all_ten_k_entries_response= await sec_home_page.content()\n",
    "\n",
    "        list_of_all_ten_k_entries_doc = BeautifulSoup(list_of_all_ten_k_entries_response, 'html.parser')\n",
    "\n",
    "        #get the first 10K\n",
    "        list_of_all_ten_k_entries_table = list_of_all_ten_k_entries_doc.find('table', class_='tableFile2')\n",
    "\n",
    "        ten_k_entries_rows = list_of_all_ten_k_entries_table.find_all('tr')  # get the first row because it has the most recent 10K\n",
    "\n",
    "        most_recent_ten_k_link = None\n",
    "\n",
    "        for one_ten_k_entries_row in ten_k_entries_rows:\n",
    "            one_ten_k_entries_row_cells = one_ten_k_entries_row.find_all('td')  # Assuming each cell is represented by a <td> tag\n",
    "        #     print(one_ten_k_entries_row_cells)\n",
    "\n",
    "            for one_ten_k_entries_row_cell in one_ten_k_entries_row_cells:\n",
    "        #         print(one_ten_k_entries_row_cell)\n",
    "                if one_ten_k_entries_row_cell.text.strip() == '10-K':\n",
    "                    ten_k_document_link_href = one_ten_k_entries_row_cell.find_next('td').find('a')['href']\n",
    "\n",
    "                    ten_k_document_link_base = \"https://www.sec.gov/\"\n",
    "                    ten_k_document_link_tail = ten_k_document_link_href\n",
    "                    ten_k_document_link = ten_k_document_link_base + ten_k_document_link_tail\n",
    "\n",
    "        #             print(document_link)\n",
    "\n",
    "                    # Store the first 10-K link and break out of the loop\n",
    "                    most_recent_ten_k_link = ten_k_document_link\n",
    "                    break\n",
    "\n",
    "            if most_recent_ten_k_link is not None:\n",
    "                break\n",
    "\n",
    "\n",
    "        # ONCE I HAVE THE TEN K DOC LINK \n",
    "        # 3. Now on the page with the document and graphics table\n",
    "\n",
    "        ten_k_doc_link_playwright = await async_playwright().start()\n",
    "        ten_k_doc_link_browser = await ten_k_doc_link_playwright.chromium.launch(headless = False)\n",
    "        ten_k_doc_link_page = await ten_k_doc_link_browser.new_page()\n",
    "\n",
    "        time.sleep(6)\n",
    "\n",
    "        await ten_k_doc_link_page.goto(most_recent_ten_k_link)\n",
    "        time.sleep(6)\n",
    "\n",
    "\n",
    "\n",
    "        # Initiate the html and beautiful soup content \n",
    "\n",
    "        list_of_doc_and_graphics_response= await ten_k_doc_link_page.content()\n",
    "\n",
    "        list_of_doc_and_graphics_soup_doc = BeautifulSoup(list_of_doc_and_graphics_response, 'html.parser')\n",
    "\n",
    "        # get actual_full_ten_k_document_link\n",
    "        list_of_doc_and_graphics_table = list_of_doc_and_graphics_soup_doc.find('table', class_='tableFile')\n",
    "\n",
    "\n",
    "        list_of_doc_and_graphics_rows = list_of_doc_and_graphics_table.find_all('tr')  # Assuming each row is represented by a <tr> tag\n",
    "\n",
    "        for list_of_doc_and_graphics_row in list_of_doc_and_graphics_rows:\n",
    "            list_of_doc_and_graphics_cells = list_of_doc_and_graphics_row.find_all('td')  # Assuming each cell is represented by a <td> tag\n",
    "        #     print(cells)\n",
    "            for list_of_doc_and_graphics_cell in list_of_doc_and_graphics_cells:\n",
    "                if list_of_doc_and_graphics_cell.text.strip() == '10-K':\n",
    "                    actual_document_link = list_of_doc_and_graphics_cell.find_next('td').find('a')\n",
    "                    if actual_document_link is not None:\n",
    "                        actual_ten_k_document_href = actual_document_link['href']\n",
    "        #                 print(\"Documents href:\", document_href)\n",
    "                        actual_ten_k_document_link_base=\"https://www.sec.gov/\"\n",
    "                        actual_ten_k_document_link_tail=actual_ten_k_document_href\n",
    "                        actual_full_ten_k_document_link=actual_ten_k_document_link_base+actual_ten_k_document_link_tail\n",
    "                        actual_full_ten_k_document_link=x\n",
    "                        print(actual_full_ten_k_document_link)\n",
    "                        print(\"#########\")\n",
    "\n",
    "                        # Add the values to the DataFrame\n",
    "                        df_stock_top_us_plus_sec_titles.loc[df_stock_top_us_plus_sec_titles['title_searchable'] == one_title_searchable, 'ten_k_document_link'] = ten_k_document_link\n",
    "                        df_stock_top_us_plus_sec_titles.loc[df_stock_top_us_plus_sec_titles['title_searchable'] == one_title_searchable, 'actual_full_ten_k_document_link'] = actual_full_ten_k_document_link\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    finally:\n",
    "        # Close the page and browser\n",
    "        await ten_k_doc_link_page.close()\n",
    "        await ten_k_doc_link_browser.close()\n",
    "        await ten_k_doc_link_playwright.stop()\n",
    "\n",
    "        # Close the sec_home page and browser\n",
    "        await sec_home_page.close()\n",
    "        await sec_home_browser.close()\n",
    "        await sec_home_playwright.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd63be40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fe322e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8f0e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cddbecf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11457f56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63fcc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_full_ten_k_document_link"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802ef491",
   "metadata": {},
   "source": [
    "# Going into link and getting stock-based compensation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5c9491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is final\n",
    "async def extracting_stock_compensation_for_one_company(one_title_searchable):\n",
    "\n",
    "    async def getting_10_k_link(one_title_searchable):\n",
    "        #### ON THE SEC HOME PAGE\n",
    "        # going to sec page\n",
    "        sec_home_playwright = await async_playwright().start()\n",
    "        sec_home_browser = await sec_home_playwright.chromium.launch(headless = False)\n",
    "        sec_home_page = await sec_home_browser.new_page()\n",
    "\n",
    "        await sec_home_page.goto(\"https://www.sec.gov/edgar/searchedgar/companysearch\")\n",
    "        time.sleep(6)\n",
    "\n",
    "        try:\n",
    "\n",
    "            # input company name (here is where we can enter title variable from df from csv)\n",
    "\n",
    "            await sec_home_page.fill(\"xpath=/html/body/div[1]/div/div/div/div[3]/div/div[2]/div[2]/div/div/article/div/div/div[1]/div[2]/div/div/div/div[1]/form/div[2]/input\", one_title_searchable)\n",
    "            time.sleep(6)\n",
    "\n",
    "            # click search\n",
    "\n",
    "            await sec_home_page.click(\"xpath=/html/body/div[1]/div/div/div/div[3]/div/div[2]/div[2]/div/div/article/div/div/div[1]/div[2]/div/div/div/div[1]/form/div[2]/button\")\n",
    "            time.sleep(6)\n",
    "\n",
    "            #============\n",
    "            # ON THE NEW PAGE RESULTING WITH ALL COMPANY DOCS\n",
    "            #input 10K for search\n",
    "            await sec_home_page.fill(\"xpath=/html/body/div[4]/div[2]/form/table/tbody/tr[3]/td[1]/input\", \"10-K\")\n",
    "            time.sleep(6)\n",
    "\n",
    "            # click search\n",
    "\n",
    "            await sec_home_page.click(\"xpath=/html/body/div[4]/div[2]/form/table/tbody/tr[3]/td[5]/input[1]\")\n",
    "            time.sleep(6)\n",
    "\n",
    "\n",
    "            #get all 10K entries\n",
    "            list_of_all_ten_k_entries_response= await sec_home_page.content()\n",
    "\n",
    "            list_of_all_ten_k_entries_doc = BeautifulSoup(list_of_all_ten_k_entries_response, 'html.parser')\n",
    "\n",
    "            #get the first 10K\n",
    "            list_of_all_ten_k_entries_table = list_of_all_ten_k_entries_doc.find('table', class_='tableFile2')\n",
    "\n",
    "            ten_k_entries_rows = list_of_all_ten_k_entries_table.find_all('tr')  # get the first row because it has the most recent 10K\n",
    "\n",
    "            most_recent_ten_k_link = None\n",
    "\n",
    "            for one_ten_k_entries_row in ten_k_entries_rows:\n",
    "                one_ten_k_entries_row_cells = one_ten_k_entries_row.find_all('td')  # Assuming each cell is represented by a <td> tag\n",
    "            #     print(one_ten_k_entries_row_cells)\n",
    "\n",
    "                for one_ten_k_entries_row_cell in one_ten_k_entries_row_cells:\n",
    "            #         print(one_ten_k_entries_row_cell)\n",
    "                    if one_ten_k_entries_row_cell.text.strip() == '10-K':\n",
    "                        ten_k_document_link_href = one_ten_k_entries_row_cell.find_next('td').find('a')['href']\n",
    "\n",
    "                        ten_k_document_link_base = \"https://www.sec.gov/\"\n",
    "                        ten_k_document_link_tail = ten_k_document_link_href\n",
    "                        ten_k_document_link = ten_k_document_link_base + ten_k_document_link_tail\n",
    "\n",
    "            #             print(document_link)\n",
    "\n",
    "                        # Store the first 10-K link and break out of the loop\n",
    "                        most_recent_ten_k_link = ten_k_document_link\n",
    "                        break\n",
    "\n",
    "                if most_recent_ten_k_link is not None:\n",
    "                    break\n",
    "\n",
    "\n",
    "            # ONCE I HAVE THE TEN K DOC LINK \n",
    "            # 3. Now on the page with the document and graphics table\n",
    "\n",
    "            ten_k_doc_link_playwright = await async_playwright().start()\n",
    "            ten_k_doc_link_browser = await ten_k_doc_link_playwright.chromium.launch(headless = False)\n",
    "            ten_k_doc_link_page = await ten_k_doc_link_browser.new_page()\n",
    "\n",
    "            time.sleep(6)\n",
    "\n",
    "            await ten_k_doc_link_page.goto(most_recent_ten_k_link)\n",
    "            time.sleep(6)\n",
    "\n",
    "\n",
    "\n",
    "            # Initiate the html and beautiful soup content \n",
    "\n",
    "            list_of_doc_and_graphics_response= await ten_k_doc_link_page.content()\n",
    "\n",
    "            list_of_doc_and_graphics_soup_doc = BeautifulSoup(list_of_doc_and_graphics_response, 'html.parser')\n",
    "\n",
    "            # get actual_full_ten_k_document_link\n",
    "            list_of_doc_and_graphics_table = list_of_doc_and_graphics_soup_doc.find('table', class_='tableFile')\n",
    "\n",
    "\n",
    "            list_of_doc_and_graphics_rows = list_of_doc_and_graphics_table.find_all('tr')  # Assuming each row is represented by a <tr> tag\n",
    "\n",
    "            for list_of_doc_and_graphics_row in list_of_doc_and_graphics_rows:\n",
    "                list_of_doc_and_graphics_cells = list_of_doc_and_graphics_row.find_all('td')  # Assuming each cell is represented by a <td> tag\n",
    "            #     print(cells)\n",
    "                for list_of_doc_and_graphics_cell in list_of_doc_and_graphics_cells:\n",
    "                    if list_of_doc_and_graphics_cell.text.strip() == '10-K':\n",
    "                        actual_document_link = list_of_doc_and_graphics_cell.find_next('td').find('a')\n",
    "                        if actual_document_link is not None:\n",
    "                            actual_ten_k_document_href = actual_document_link['href']\n",
    "            #                 print(\"Documents href:\", document_href)\n",
    "                            actual_ten_k_document_link_base=\"https://www.sec.gov/\"\n",
    "                            actual_ten_k_document_link_tail=actual_ten_k_document_href\n",
    "                            actual_full_ten_k_document_link=actual_ten_k_document_link_base+actual_ten_k_document_link_tail\n",
    "#                             global x\n",
    "                            x=actual_full_ten_k_document_link\n",
    "                            print(actual_full_ten_k_document_link)\n",
    "                            print(\"#########\")\n",
    "\n",
    "                            \n",
    "\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        finally:\n",
    "            # Close the page and browser\n",
    "            await ten_k_doc_link_page.close()\n",
    "            await ten_k_doc_link_browser.close()\n",
    "            await ten_k_doc_link_playwright.stop()\n",
    "\n",
    "            # Close the sec_home page and browser\n",
    "            await sec_home_page.close()\n",
    "            await sec_home_browser.close()\n",
    "            await sec_home_playwright.stop()\n",
    "            return actual_full_ten_k_document_link\n",
    "    \n",
    "    \n",
    "    \n",
    "    x=one_title_searchable\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # with change for link\n",
    "\n",
    "\n",
    "    list_of_actual_full_ten_k_document_links = [\"https://www.sec.gov/ix?doc=/Archives/edgar/data/789019/000156459022026876/msft-10k_20220630.htm\"]\n",
    "\n",
    "    actual_list_of_actual_full_ten_k_document_links = [\"https://www.sec.gov//ix?doc=/Archives/edgar/data/320193/000032019322000108/aapl-20220924.htm\",\"https://www.sec.gov/ix?doc=/Archives/edgar/data/789019/000156459022026876/msft-10k_20220630.htm\",\"https://www.sec.gov/ix?doc=/Archives/edgar/data/1652044/000165204423000016/goog-20221231.htm\"]\n",
    "\n",
    "    async def get_actual_full_ten_k_document_content(x):\n",
    "        playwright = await async_playwright().start()\n",
    "        browser = await playwright.chromium.launch(headless=False)\n",
    "    #     browser = await playwright.firefox.launch(headless=False)\n",
    "\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        await page.goto(x)\n",
    "        time.sleep(10)\n",
    "\n",
    "        while True:\n",
    "            # Get the number of spans on the page\n",
    "            spans_assigned = await page.query_selector_all(\"span\")\n",
    "            test = len(spans_assigned)\n",
    "\n",
    "            # Scroll in increments of 5000 pixels\n",
    "            await page.evaluate('''() => {\n",
    "                let distance = 5000;\n",
    "                window.scrollBy(0, distance);\n",
    "            }''')\n",
    "    #         await asyncio.sleep(3)\n",
    "\n",
    "            time.sleep(10)\n",
    "\n",
    "\n",
    "            spans_assigned_two = await page.query_selector_all(\"span\")\n",
    "            test2 = len(spans_assigned_two)\n",
    "            if test == test2:\n",
    "                break\n",
    "\n",
    "        # Get the content of the page\n",
    "        actual_full_ten_k_document_content = await page.content()\n",
    "        time.sleep(10)\n",
    "\n",
    "        await browser.close()\n",
    "        time.sleep(10)\n",
    "        await page.close()  # Add this line to close the page\n",
    "        time.sleep(10)\n",
    "\n",
    "        return actual_full_ten_k_document_content\n",
    "\n",
    "\n",
    "    async def process_actual_full_ten_k_document(actual_full_ten_k_document_content):\n",
    "        actual_full_ten_k_document_soup = BeautifulSoup(actual_full_ten_k_document_content, 'html.parser')\n",
    "        return actual_full_ten_k_document_soup\n",
    "\n",
    "\n",
    "    async def main():\n",
    "        dict_titles=['actual_full_ten_k_document_link_collection',\n",
    "                     'table_length',\n",
    "                     'list_of_stock_based_values',\n",
    "                     'first_stock_based_compensation',\n",
    "                     'second_stock_based_compensation',\n",
    "                     'third_stock_based_compensation']\n",
    "\n",
    "        possible_titles = ['CONSOLIDATED STATEMENTS OF CASH FLOWS',\n",
    "                           'CASH FLOWS STATEMENTS',\n",
    "                           'Consolidated Statements of Cash Flows',\n",
    "                           'Consolidated Statement of Cash Flows',\n",
    "                          'CONSOLIDATED STATEMENT OF CASH FLOWS',\n",
    "                          'STATEMENTS OF CONSOLIDATED CASH FLOWS',\n",
    "                          'Consolidated Cash Flow Statement',\n",
    "                          'Consolidated Statement of Cash Flow for the Years Ended December 31',\n",
    "                          'STATEMENT OF CASH FLOWS',\n",
    "                          'CONSOLIDATED STATEMENTS OF CASH FLOW',\n",
    "                          'Statements of Consolidated Cash Flows']\n",
    "\n",
    "        stock_related_texts = [\n",
    "            'Stock-based compensation expense',\n",
    "            'Stock-based compensation expense',\n",
    "            'Share-based compensation expense',\n",
    "            'Stock-based compensation',\n",
    "            'Share-based compensation'\n",
    "        ]\n",
    "\n",
    "        all_ten_k_document_lists = []\n",
    "\n",
    "\n",
    "    #     list_of_example_actual_full_ten_k_document_soup_doc = []\n",
    "\n",
    "        # Define the group size\n",
    "        group_size = 2\n",
    "        items=[x]\n",
    "\n",
    "        # Iterate over groups of items\n",
    "        for start_index in range(0, len(items), group_size):\n",
    "            group_items = items[start_index:start_index + group_size]\n",
    "\n",
    "            list_of_example_actual_full_ten_k_document_soup_doc = []\n",
    "\n",
    "\n",
    "    #         # Iterate over items within the group\n",
    "    #         for item in group_items:\n",
    "    #             print(item)\n",
    "\n",
    "    #         # Add a separator for each group\n",
    "    #         print(\"Group separator\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            url_to_parsed_content = {}\n",
    "\n",
    "\n",
    "            for actual_full_ten_k_document_link in group_items:\n",
    "                try:\n",
    "                    actual_full_ten_k_document_link_collection = actual_full_ten_k_document_link\n",
    "                    print(actual_full_ten_k_document_link)\n",
    "\n",
    "                    for _ in range(3):\n",
    "                        try:\n",
    "                            actual_full_ten_k_document_content = await get_actual_full_ten_k_document_content(actual_full_ten_k_document_link)\n",
    "                            example_actual_full_ten_k_document_soup_doc = await process_actual_full_ten_k_document(actual_full_ten_k_document_content)\n",
    "                            list_of_example_actual_full_ten_k_document_soup_doc.append(example_actual_full_ten_k_document_soup_doc)\n",
    "                                # Store the parsed content in the dictionary.\n",
    "                            url_to_parsed_content[actual_full_ten_k_document_link] = example_actual_full_ten_k_document_soup_doc\n",
    "\n",
    "                            break  # Break the loop if successful\n",
    "                        except Exception as e:\n",
    "                            print(f\"An error occurred: {str(e)}. Retrying...\")\n",
    "                            time.sleep(15)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            for actual_full_ten_k_document_link, one_actual_full_ten_k_document_soup_doc in url_to_parsed_content.items():\n",
    "                ten_k_detail_list=[]\n",
    "\n",
    "                if one_actual_full_ten_k_document_soup_doc:\n",
    "\n",
    "                    elements = one_actual_full_ten_k_document_soup_doc.find_all(text=possible_titles)\n",
    "\n",
    "                    if not elements:\n",
    "                        print(\"Titles not found in the HTML.\")\n",
    "                        continue\n",
    "\n",
    "                    for element in elements:\n",
    "                        if element.strip() in possible_titles:\n",
    "                            following_elements = element.find_all_next()\n",
    "                            table = None\n",
    "\n",
    "                            # Iterate through following elements and stop when a table is found or after three components\n",
    "                            component_count = 0\n",
    "                            for next_element in following_elements:\n",
    "                                if next_element.name == 'table' and any(text in next_element.get_text() for text in stock_related_texts):\n",
    "                                    table = next_element\n",
    "                                    break\n",
    "                                elif next_element.name in ['p', 'span']:\n",
    "                                    component_count += 1\n",
    "                                    if component_count >= 5:\n",
    "                                        break\n",
    "\n",
    "                            if table is None:\n",
    "                                tables = one_actual_full_ten_k_document_soup_doc.find_all(\"table\")\n",
    "                                for t in tables:     \n",
    "                                    if t:\n",
    "                                        table_text = t.get_text()\n",
    "                                        if any(title in table_text for title in possible_titles) and any(stock_related_text in table_text for stock_related_text in stock_related_texts):\n",
    "                                            rows = t.find_all('tr')\n",
    "                                            table_length = len(rows)\n",
    "                                            print(f\"Table Length from within: {len(rows)}\") \n",
    "                                        break\n",
    "\n",
    "                            if table:\n",
    "                                rows = table.find_all('tr')\n",
    "                                table_length=len(rows)\n",
    "\n",
    "                                print(f\"Table Length: {len(rows)}\")                        \n",
    "\n",
    "                                break\n",
    "\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        print(\"No table found within or after the possible titles in the HTML.\")\n",
    "\n",
    "                else:\n",
    "                    print(\"Document not processed, skipping\")\n",
    "\n",
    "\n",
    "                if table:\n",
    "                    target_td = table.find('td', text=stock_related_texts)\n",
    "                else:\n",
    "                    target_td = None\n",
    "\n",
    "                if target_td:\n",
    "                    # Move up to the <tr> containing the <td> with \"share-based compensation\"\n",
    "                    target_tr = target_td.find_parent('tr')\n",
    "\n",
    "                    # Extract all the <td> values from the <tr> and put them in a column\n",
    "                    values_in_column = [td.get_text(strip=True) for td in target_tr.find_all('td')]\n",
    "\n",
    "                    # Output the values in the column\n",
    "                    if values_in_column:\n",
    "                        stock_stock_based_compensation_title=values_in_column[0]\n",
    "                        list_of_stock_based_values=values_in_column\n",
    "                        numbers_only_values_in_column = [entry for entry in values_in_column if any(char.isdigit() for char in entry)]\n",
    "                        first_stock_based_compensation=numbers_only_values_in_column[0]\n",
    "                        second_stock_based_compensation=numbers_only_values_in_column[1]\n",
    "                        third_stock_based_compensation=numbers_only_values_in_column[2]\n",
    "                        for value in values_in_column:\n",
    "                            print(value)\n",
    "\n",
    "                    ten_k_detail_list.append(actual_full_ten_k_document_link)\n",
    "                    ten_k_detail_list.append(table_length)\n",
    "                    ten_k_detail_list.append(list_of_stock_based_values)\n",
    "                    ten_k_detail_list.append(first_stock_based_compensation)\n",
    "                    ten_k_detail_list.append(second_stock_based_compensation)\n",
    "                    ten_k_detail_list.append(third_stock_based_compensation)\n",
    "\n",
    "                    first_rep = dict(zip(dict_titles, ten_k_detail_list))\n",
    "                    all_ten_k_document_lists.append(first_rep)\n",
    "\n",
    "\n",
    "\n",
    "                else:\n",
    "                    print(\"The 'Share-Based Compensation' row not found!\")\n",
    "                time.sleep(10)\n",
    "\n",
    "    #     print(all_ten_k_document_lists)\n",
    "        return all_ten_k_document_lists\n",
    "\n",
    "\n",
    "    # Run the main coroutine\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    # Use asyncio.run() to run the main coroutine\n",
    "    all_ten_k_document_lists_of_dicts=asyncio.run(main())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ac7548",
   "metadata": {},
   "outputs": [],
   "source": [
    "await extracting_stock_compensation_for_one_company(\"https://www.sec.gov//ix?doc=/Archives/edgar/data/320193/000032019322000108/aapl-20220924.htm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27554553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63620b97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c2f076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac0a4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b78ee2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e161e4d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4655671",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5b4f66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ce2adc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8374b709",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d1f173",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7385001c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfc216d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49afa807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is final\n",
    "async def extracting_stock_compensation_for_one_company(one_title_searchable):\n",
    "\n",
    "    async def getting_10_k_link(one_title_searchable):\n",
    "        #### ON THE SEC HOME PAGE\n",
    "        # going to sec page\n",
    "        sec_home_playwright = await async_playwright().start()\n",
    "        sec_home_browser = await sec_home_playwright.chromium.launch(headless = False)\n",
    "        sec_home_page = await sec_home_browser.new_page()\n",
    "\n",
    "        await sec_home_page.goto(\"https://www.sec.gov/edgar/searchedgar/companysearch\")\n",
    "        time.sleep(6)\n",
    "\n",
    "        try:\n",
    "\n",
    "            # input company name (here is where we can enter title variable from df from csv)\n",
    "\n",
    "            await sec_home_page.fill(\"xpath=/html/body/div[1]/div/div/div/div[3]/div/div[2]/div[2]/div/div/article/div/div/div[1]/div[2]/div/div/div/div[1]/form/div[2]/input\", one_title_searchable)\n",
    "            time.sleep(6)\n",
    "\n",
    "            # click search\n",
    "\n",
    "            await sec_home_page.click(\"xpath=/html/body/div[1]/div/div/div/div[3]/div/div[2]/div[2]/div/div/article/div/div/div[1]/div[2]/div/div/div/div[1]/form/div[2]/button\")\n",
    "            time.sleep(6)\n",
    "\n",
    "            #============\n",
    "            # ON THE NEW PAGE RESULTING WITH ALL COMPANY DOCS\n",
    "            #input 10K for search\n",
    "            await sec_home_page.fill(\"xpath=/html/body/div[4]/div[2]/form/table/tbody/tr[3]/td[1]/input\", \"10-K\")\n",
    "            time.sleep(6)\n",
    "\n",
    "            # click search\n",
    "\n",
    "            await sec_home_page.click(\"xpath=/html/body/div[4]/div[2]/form/table/tbody/tr[3]/td[5]/input[1]\")\n",
    "            time.sleep(6)\n",
    "\n",
    "\n",
    "            #get all 10K entries\n",
    "            list_of_all_ten_k_entries_response= await sec_home_page.content()\n",
    "\n",
    "            list_of_all_ten_k_entries_doc = BeautifulSoup(list_of_all_ten_k_entries_response, 'html.parser')\n",
    "\n",
    "            #get the first 10K\n",
    "            list_of_all_ten_k_entries_table = list_of_all_ten_k_entries_doc.find('table', class_='tableFile2')\n",
    "\n",
    "            ten_k_entries_rows = list_of_all_ten_k_entries_table.find_all('tr')  # get the first row because it has the most recent 10K\n",
    "\n",
    "            most_recent_ten_k_link = None\n",
    "\n",
    "            for one_ten_k_entries_row in ten_k_entries_rows:\n",
    "                one_ten_k_entries_row_cells = one_ten_k_entries_row.find_all('td')  # Assuming each cell is represented by a <td> tag\n",
    "            #     print(one_ten_k_entries_row_cells)\n",
    "\n",
    "                for one_ten_k_entries_row_cell in one_ten_k_entries_row_cells:\n",
    "            #         print(one_ten_k_entries_row_cell)\n",
    "                    if one_ten_k_entries_row_cell.text.strip() == '10-K':\n",
    "                        ten_k_document_link_href = one_ten_k_entries_row_cell.find_next('td').find('a')['href']\n",
    "\n",
    "                        ten_k_document_link_base = \"https://www.sec.gov/\"\n",
    "                        ten_k_document_link_tail = ten_k_document_link_href\n",
    "                        ten_k_document_link = ten_k_document_link_base + ten_k_document_link_tail\n",
    "\n",
    "            #             print(document_link)\n",
    "\n",
    "                        # Store the first 10-K link and break out of the loop\n",
    "                        most_recent_ten_k_link = ten_k_document_link\n",
    "                        break\n",
    "\n",
    "                if most_recent_ten_k_link is not None:\n",
    "                    break\n",
    "\n",
    "\n",
    "            # ONCE I HAVE THE TEN K DOC LINK \n",
    "            # 3. Now on the page with the document and graphics table\n",
    "\n",
    "            ten_k_doc_link_playwright = await async_playwright().start()\n",
    "            ten_k_doc_link_browser = await ten_k_doc_link_playwright.chromium.launch(headless = False)\n",
    "            ten_k_doc_link_page = await ten_k_doc_link_browser.new_page()\n",
    "\n",
    "            time.sleep(6)\n",
    "\n",
    "            await ten_k_doc_link_page.goto(most_recent_ten_k_link)\n",
    "            time.sleep(6)\n",
    "\n",
    "\n",
    "\n",
    "            # Initiate the html and beautiful soup content \n",
    "\n",
    "            list_of_doc_and_graphics_response= await ten_k_doc_link_page.content()\n",
    "\n",
    "            list_of_doc_and_graphics_soup_doc = BeautifulSoup(list_of_doc_and_graphics_response, 'html.parser')\n",
    "\n",
    "            # get actual_full_ten_k_document_link\n",
    "            list_of_doc_and_graphics_table = list_of_doc_and_graphics_soup_doc.find('table', class_='tableFile')\n",
    "\n",
    "\n",
    "            list_of_doc_and_graphics_rows = list_of_doc_and_graphics_table.find_all('tr')  # Assuming each row is represented by a <tr> tag\n",
    "\n",
    "            for list_of_doc_and_graphics_row in list_of_doc_and_graphics_rows:\n",
    "                list_of_doc_and_graphics_cells = list_of_doc_and_graphics_row.find_all('td')  # Assuming each cell is represented by a <td> tag\n",
    "            #     print(cells)\n",
    "                for list_of_doc_and_graphics_cell in list_of_doc_and_graphics_cells:\n",
    "                    if list_of_doc_and_graphics_cell.text.strip() == '10-K':\n",
    "                        actual_document_link = list_of_doc_and_graphics_cell.find_next('td').find('a')\n",
    "                        if actual_document_link is not None:\n",
    "                            actual_ten_k_document_href = actual_document_link['href']\n",
    "            #                 print(\"Documents href:\", document_href)\n",
    "                            actual_ten_k_document_link_base=\"https://www.sec.gov/\"\n",
    "                            actual_ten_k_document_link_tail=actual_ten_k_document_href\n",
    "                            actual_full_ten_k_document_link=actual_ten_k_document_link_base+actual_ten_k_document_link_tail\n",
    "                            x=actual_full_ten_k_document_link\n",
    "                            print(actual_full_ten_k_document_link)\n",
    "                            print(\"#########\")\n",
    "\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        finally:\n",
    "            # Close the page and browser\n",
    "            await ten_k_doc_link_page.close()\n",
    "            await ten_k_doc_link_browser.close()\n",
    "            await ten_k_doc_link_playwright.stop()\n",
    "\n",
    "            # Close the sec_home page and browser\n",
    "            await sec_home_page.close()\n",
    "            await sec_home_browser.close()\n",
    "            await sec_home_playwright.stop()\n",
    "        return x\n",
    "\n",
    "    # with change for link\n",
    "\n",
    "\n",
    "    list_of_actual_full_ten_k_document_links = [\"https://www.sec.gov/ix?doc=/Archives/edgar/data/789019/000156459022026876/msft-10k_20220630.htm\"]\n",
    "\n",
    "    actual_list_of_actual_full_ten_k_document_links = [\"https://www.sec.gov//ix?doc=/Archives/edgar/data/320193/000032019322000108/aapl-20220924.htm\",\"https://www.sec.gov/ix?doc=/Archives/edgar/data/789019/000156459022026876/msft-10k_20220630.htm\",\"https://www.sec.gov/ix?doc=/Archives/edgar/data/1652044/000165204423000016/goog-20221231.htm\"]\n",
    "\n",
    "    async def get_actual_full_ten_k_document_content(x):\n",
    "        playwright = await async_playwright().start()\n",
    "        browser = await playwright.chromium.launch(headless=False)\n",
    "    #     browser = await playwright.firefox.launch(headless=False)\n",
    "\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        await page.goto(x)\n",
    "        time.sleep(10)\n",
    "\n",
    "        while True:\n",
    "            # Get the number of spans on the page\n",
    "            spans_assigned = await page.query_selector_all(\"span\")\n",
    "            test = len(spans_assigned)\n",
    "\n",
    "            # Scroll in increments of 5000 pixels\n",
    "            await page.evaluate('''() => {\n",
    "                let distance = 5000;\n",
    "                window.scrollBy(0, distance);\n",
    "            }''')\n",
    "    #         await asyncio.sleep(3)\n",
    "\n",
    "            time.sleep(10)\n",
    "\n",
    "\n",
    "            spans_assigned_two = await page.query_selector_all(\"span\")\n",
    "            test2 = len(spans_assigned_two)\n",
    "            if test == test2:\n",
    "                break\n",
    "\n",
    "        # Get the content of the page\n",
    "        actual_full_ten_k_document_content = await page.content()\n",
    "        time.sleep(10)\n",
    "\n",
    "        await browser.close()\n",
    "        time.sleep(10)\n",
    "        await page.close()  # Add this line to close the page\n",
    "        time.sleep(10)\n",
    "\n",
    "        return actual_full_ten_k_document_content\n",
    "\n",
    "\n",
    "    async def process_actual_full_ten_k_document(actual_full_ten_k_document_content):\n",
    "        actual_full_ten_k_document_soup = BeautifulSoup(actual_full_ten_k_document_content, 'html.parser')\n",
    "        return actual_full_ten_k_document_soup\n",
    "\n",
    "\n",
    "    async def main():\n",
    "        dict_titles=['actual_full_ten_k_document_link_collection',\n",
    "                     'table_length',\n",
    "                     'list_of_stock_based_values',\n",
    "                     'first_stock_based_compensation',\n",
    "                     'second_stock_based_compensation',\n",
    "                     'third_stock_based_compensation']\n",
    "\n",
    "        possible_titles = ['CONSOLIDATED STATEMENTS OF CASH FLOWS',\n",
    "                           'CASH FLOWS STATEMENTS',\n",
    "                           'Consolidated Statements of Cash Flows',\n",
    "                           'Consolidated Statement of Cash Flows',\n",
    "                          'CONSOLIDATED STATEMENT OF CASH FLOWS',\n",
    "                          'STATEMENTS OF CONSOLIDATED CASH FLOWS',\n",
    "                          'Consolidated Cash Flow Statement',\n",
    "                          'Consolidated Statement of Cash Flow for the Years Ended December 31',\n",
    "                          'STATEMENT OF CASH FLOWS',\n",
    "                          'CONSOLIDATED STATEMENTS OF CASH FLOW',\n",
    "                          'Statements of Consolidated Cash Flows']\n",
    "\n",
    "        stock_related_texts = [\n",
    "            'Stock-based compensation expense',\n",
    "            'Stock-based compensation expense',\n",
    "            'Share-based compensation expense',\n",
    "            'Stock-based compensation',\n",
    "            'Share-based compensation'\n",
    "        ]\n",
    "\n",
    "        all_ten_k_document_lists = []\n",
    "\n",
    "\n",
    "    #     list_of_example_actual_full_ten_k_document_soup_doc = []\n",
    "\n",
    "        # Define the group size\n",
    "        group_size = 2\n",
    "        items=[x]\n",
    "\n",
    "        # Iterate over groups of items\n",
    "        for start_index in range(0, len(items), group_size):\n",
    "            group_items = items[start_index:start_index + group_size]\n",
    "\n",
    "            list_of_example_actual_full_ten_k_document_soup_doc = []\n",
    "\n",
    "\n",
    "    #         # Iterate over items within the group\n",
    "    #         for item in group_items:\n",
    "    #             print(item)\n",
    "\n",
    "    #         # Add a separator for each group\n",
    "    #         print(\"Group separator\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            url_to_parsed_content = {}\n",
    "\n",
    "\n",
    "            for actual_full_ten_k_document_link in group_items:\n",
    "                try:\n",
    "                    actual_full_ten_k_document_link_collection = actual_full_ten_k_document_link\n",
    "                    print(actual_full_ten_k_document_link)\n",
    "\n",
    "                    for _ in range(3):\n",
    "                        try:\n",
    "                            actual_full_ten_k_document_content = await get_actual_full_ten_k_document_content(actual_full_ten_k_document_link)\n",
    "                            example_actual_full_ten_k_document_soup_doc = await process_actual_full_ten_k_document(actual_full_ten_k_document_content)\n",
    "                            list_of_example_actual_full_ten_k_document_soup_doc.append(example_actual_full_ten_k_document_soup_doc)\n",
    "                                # Store the parsed content in the dictionary.\n",
    "                            url_to_parsed_content[actual_full_ten_k_document_link] = example_actual_full_ten_k_document_soup_doc\n",
    "\n",
    "                            break  # Break the loop if successful\n",
    "                        except Exception as e:\n",
    "                            print(f\"An error occurred: {str(e)}. Retrying...\")\n",
    "                            time.sleep(15)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            for actual_full_ten_k_document_link, one_actual_full_ten_k_document_soup_doc in url_to_parsed_content.items():\n",
    "                ten_k_detail_list=[]\n",
    "\n",
    "                elements = one_actual_full_ten_k_document_soup_doc.find_all(text=possible_titles)\n",
    "\n",
    "                if not elements:\n",
    "                    print(\"Titles not found in the HTML.\")\n",
    "                    continue\n",
    "\n",
    "                for element in elements:\n",
    "                    if element.strip() in possible_titles:\n",
    "                        following_elements = element.find_all_next()\n",
    "                        table = None\n",
    "\n",
    "                        # Iterate through following elements and stop when a table is found or after three components\n",
    "                        component_count = 0\n",
    "                        for next_element in following_elements:\n",
    "                            if next_element.name == 'table' and any(text in next_element.get_text() for text in stock_related_texts):\n",
    "                                table = next_element\n",
    "                                break\n",
    "                            elif next_element.name in ['p', 'span']:\n",
    "                                component_count += 1\n",
    "                                if component_count >= 5:\n",
    "                                    break\n",
    "\n",
    "                        if table:\n",
    "                            rows = table.find_all('tr')\n",
    "                            table_length=len(rows)\n",
    "\n",
    "                            print(f\"Table Length: {len(rows)}\")                        \n",
    "\n",
    "                            break\n",
    "\n",
    "                else:\n",
    "                    print(\"No table found after the possible titles in the HTML.\")\n",
    "\n",
    "                if table:\n",
    "                    target_td = table.find('td', text=stock_related_texts)\n",
    "                else:\n",
    "                    target_td = None\n",
    "\n",
    "                if target_td:\n",
    "                    # Move up to the <tr> containing the <td> with \"share-based compensation\"\n",
    "                    target_tr = target_td.find_parent('tr')\n",
    "\n",
    "                    # Extract all the <td> values from the <tr> and put them in a column\n",
    "                    values_in_column = [td.get_text(strip=True) for td in target_tr.find_all('td')]\n",
    "\n",
    "                    # Output the values in the column\n",
    "                    if values_in_column:\n",
    "                        stock_stock_based_compensation_title=values_in_column[0]\n",
    "                        list_of_stock_based_values=values_in_column\n",
    "                        numbers_only_values_in_column = [entry for entry in values_in_column if any(char.isdigit() for char in entry)]\n",
    "                        first_stock_based_compensation=numbers_only_values_in_column[0]\n",
    "                        second_stock_based_compensation=numbers_only_values_in_column[1]\n",
    "                        third_stock_based_compensation=numbers_only_values_in_column[2]\n",
    "                        for value in values_in_column:\n",
    "                            print(value)\n",
    "\n",
    "                    ten_k_detail_list.append(actual_full_ten_k_document_link)\n",
    "                    ten_k_detail_list.append(table_length)\n",
    "                    ten_k_detail_list.append(list_of_stock_based_values)\n",
    "                    ten_k_detail_list.append(first_stock_based_compensation)\n",
    "                    ten_k_detail_list.append(second_stock_based_compensation)\n",
    "                    ten_k_detail_list.append(third_stock_based_compensation)\n",
    "\n",
    "                    first_rep = dict(zip(dict_titles, ten_k_detail_list))\n",
    "                    all_ten_k_document_lists.append(first_rep)\n",
    "\n",
    "\n",
    "\n",
    "                else:\n",
    "                    print(\"The 'Share-Based Compensation' row not found!\")\n",
    "                time.sleep(10)\n",
    "\n",
    "    #     print(all_ten_k_document_lists)\n",
    "        return all_ten_k_document_lists\n",
    "\n",
    "\n",
    "    # Run the main coroutine\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    # Use asyncio.run() to run the main coroutine\n",
    "    all_ten_k_document_lists_of_dicts=asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a104ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "await extracting_stock_compensation_for_one_company(\"https://www.sec.gov//ix?doc=/Archives/edgar/data/320193/000032019322000108/aapl-20220924.htm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e3574b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9610c56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fcf76e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b40742",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8818f38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164d5d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with change for link\n",
    "x=actual_full_ten_k_document_link\n",
    "\n",
    "list_of_actual_full_ten_k_document_links = [\"https://www.sec.gov/ix?doc=/Archives/edgar/data/789019/000156459022026876/msft-10k_20220630.htm\"]\n",
    "\n",
    "actual_list_of_actual_full_ten_k_document_links = [\"https://www.sec.gov//ix?doc=/Archives/edgar/data/320193/000032019322000108/aapl-20220924.htm\",\"https://www.sec.gov/ix?doc=/Archives/edgar/data/789019/000156459022026876/msft-10k_20220630.htm\",\"https://www.sec.gov/ix?doc=/Archives/edgar/data/1652044/000165204423000016/goog-20221231.htm\"]\n",
    "\n",
    "async def get_actual_full_ten_k_document_content(x):\n",
    "    playwright = await async_playwright().start()\n",
    "    browser = await playwright.chromium.launch(headless=False)\n",
    "#     browser = await playwright.firefox.launch(headless=False)\n",
    "\n",
    "    page = await browser.new_page()\n",
    "\n",
    "    await page.goto(x)\n",
    "    time.sleep(10)\n",
    "\n",
    "    while True:\n",
    "        # Get the number of spans on the page\n",
    "        spans_assigned = await page.query_selector_all(\"span\")\n",
    "        test = len(spans_assigned)\n",
    "\n",
    "        # Scroll in increments of 5000 pixels\n",
    "        await page.evaluate('''() => {\n",
    "            let distance = 5000;\n",
    "            window.scrollBy(0, distance);\n",
    "        }''')\n",
    "#         await asyncio.sleep(3)\n",
    "\n",
    "        time.sleep(10)\n",
    "\n",
    "\n",
    "        spans_assigned_two = await page.query_selector_all(\"span\")\n",
    "        test2 = len(spans_assigned_two)\n",
    "        if test == test2:\n",
    "            break\n",
    "\n",
    "    # Get the content of the page\n",
    "    actual_full_ten_k_document_content = await page.content()\n",
    "    time.sleep(10)\n",
    "\n",
    "    await browser.close()\n",
    "    time.sleep(10)\n",
    "    await page.close()  # Add this line to close the page\n",
    "    time.sleep(10)\n",
    "    \n",
    "    return actual_full_ten_k_document_content\n",
    "\n",
    "\n",
    "async def process_actual_full_ten_k_document(actual_full_ten_k_document_content):\n",
    "    actual_full_ten_k_document_soup = BeautifulSoup(actual_full_ten_k_document_content, 'html.parser')\n",
    "    return actual_full_ten_k_document_soup\n",
    "\n",
    "\n",
    "async def main():\n",
    "    dict_titles=['actual_full_ten_k_document_link_collection',\n",
    "                 'table_length',\n",
    "                 'list_of_stock_based_values',\n",
    "                 'first_stock_based_compensation',\n",
    "                 'second_stock_based_compensation',\n",
    "                 'third_stock_based_compensation']\n",
    "    \n",
    "    possible_titles = ['CONSOLIDATED STATEMENTS OF CASH FLOWS',\n",
    "                       'CASH FLOWS STATEMENTS',\n",
    "                       'Consolidated Statements of Cash Flows',\n",
    "                       'Consolidated Statement of Cash Flows',\n",
    "                      'CONSOLIDATED STATEMENT OF CASH FLOWS',\n",
    "                      'STATEMENTS OF CONSOLIDATED CASH FLOWS',\n",
    "                      'Consolidated Cash Flow Statement',\n",
    "                      'Consolidated Statement of Cash Flow for the Years Ended December 31',\n",
    "                      'STATEMENT OF CASH FLOWS',\n",
    "                      'CONSOLIDATED STATEMENTS OF CASH FLOW',\n",
    "                      'Statements of Consolidated Cash Flows']\n",
    "\n",
    "    stock_related_texts = [\n",
    "        'Stock-based compensation expense',\n",
    "        'Stock-based compensation expense',\n",
    "        'Share-based compensation expense',\n",
    "        'Stock-based compensation',\n",
    "        'Share-based compensation'\n",
    "    ]\n",
    "\n",
    "    all_ten_k_document_lists = []\n",
    "\n",
    "\n",
    "#     list_of_example_actual_full_ten_k_document_soup_doc = []\n",
    "\n",
    "    # Define the group size\n",
    "    group_size = 1\n",
    "    items=[x]\n",
    "    \n",
    "    # Iterate over groups of items\n",
    "    for start_index in range(0, len(items), group_size):\n",
    "        group_items = items[start_index:start_index + group_size]\n",
    "        \n",
    "        list_of_example_actual_full_ten_k_document_soup_doc = []\n",
    "\n",
    "\n",
    "#         # Iterate over items within the group\n",
    "#         for item in group_items:\n",
    "#             print(item)\n",
    "\n",
    "#         # Add a separator for each group\n",
    "#         print(\"Group separator\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        url_to_parsed_content = {}\n",
    "\n",
    "\n",
    "        for actual_full_ten_k_document_link in group_items:\n",
    "            try:\n",
    "                actual_full_ten_k_document_link_collection = actual_full_ten_k_document_link\n",
    "                print(actual_full_ten_k_document_link)\n",
    "\n",
    "                for _ in range(3):\n",
    "                    try:\n",
    "                        actual_full_ten_k_document_content = await get_actual_full_ten_k_document_content(actual_full_ten_k_document_link)\n",
    "                        example_actual_full_ten_k_document_soup_doc = await process_actual_full_ten_k_document(actual_full_ten_k_document_content)\n",
    "                        list_of_example_actual_full_ten_k_document_soup_doc.append(example_actual_full_ten_k_document_soup_doc)\n",
    "                            # Store the parsed content in the dictionary.\n",
    "                        url_to_parsed_content[actual_full_ten_k_document_link] = example_actual_full_ten_k_document_soup_doc\n",
    "\n",
    "                        break  # Break the loop if successful\n",
    "                    except Exception as e:\n",
    "                        print(f\"An error occurred: {str(e)}. Retrying...\")\n",
    "                        time.sleep(15)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        for actual_full_ten_k_document_link, one_actual_full_ten_k_document_soup_doc in url_to_parsed_content.items():\n",
    "            ten_k_detail_list=[]\n",
    "\n",
    "            elements = one_actual_full_ten_k_document_soup_doc.find_all(text=possible_titles)\n",
    "\n",
    "            if not elements:\n",
    "                print(\"Titles not found in the HTML.\")\n",
    "                continue\n",
    "\n",
    "            for element in elements:\n",
    "                if element.strip() in possible_titles:\n",
    "                    following_elements = element.find_all_next()\n",
    "                    table = None\n",
    "\n",
    "                    # Iterate through following elements and stop when a table is found or after three components\n",
    "                    component_count = 0\n",
    "                    for next_element in following_elements:\n",
    "                        if next_element.name == 'table' and any(text in next_element.get_text() for text in stock_related_texts):\n",
    "                            table = next_element\n",
    "                            break\n",
    "                        elif next_element.name in ['p', 'span']:\n",
    "                            component_count += 1\n",
    "                            if component_count >= 5:\n",
    "                                break\n",
    "\n",
    "                    if table:\n",
    "                        rows = table.find_all('tr')\n",
    "                        table_length=len(rows)\n",
    "\n",
    "                        print(f\"Table Length: {len(rows)}\")                        \n",
    "\n",
    "                        break\n",
    "\n",
    "            else:\n",
    "                print(\"No table found after the possible titles in the HTML.\")\n",
    "\n",
    "            if table:\n",
    "                target_td = table.find('td', text=stock_related_texts)\n",
    "            else:\n",
    "                target_td = None\n",
    "\n",
    "            if target_td:\n",
    "                # Move up to the <tr> containing the <td> with \"share-based compensation\"\n",
    "                target_tr = target_td.find_parent('tr')\n",
    "\n",
    "                # Extract all the <td> values from the <tr> and put them in a column\n",
    "                values_in_column = [td.get_text(strip=True) for td in target_tr.find_all('td')]\n",
    "\n",
    "                # Output the values in the column\n",
    "                if values_in_column:\n",
    "                    stock_stock_based_compensation_title=values_in_column[0]\n",
    "                    list_of_stock_based_values=values_in_column\n",
    "                    numbers_only_values_in_column = [entry for entry in values_in_column if any(char.isdigit() for char in entry)]\n",
    "                    first_stock_based_compensation=numbers_only_values_in_column[0]\n",
    "                    second_stock_based_compensation=numbers_only_values_in_column[1]\n",
    "                    third_stock_based_compensation=numbers_only_values_in_column[2]\n",
    "                    for value in values_in_column:\n",
    "                        print(value)\n",
    "\n",
    "                ten_k_detail_list.append(actual_full_ten_k_document_link)\n",
    "                ten_k_detail_list.append(table_length)\n",
    "                ten_k_detail_list.append(list_of_stock_based_values)\n",
    "                ten_k_detail_list.append(first_stock_based_compensation)\n",
    "                ten_k_detail_list.append(second_stock_based_compensation)\n",
    "                ten_k_detail_list.append(third_stock_based_compensation)\n",
    "\n",
    "                first_rep = dict(zip(dict_titles, ten_k_detail_list))\n",
    "                all_ten_k_document_lists.append(first_rep)\n",
    "\n",
    "\n",
    "\n",
    "            else:\n",
    "                print(\"The 'Share-Based Compensation' row not found!\")\n",
    "            time.sleep(10)\n",
    "\n",
    "#     print(all_ten_k_document_lists)\n",
    "    return all_ten_k_document_lists\n",
    "\n",
    "\n",
    "# Run the main coroutine\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Use asyncio.run() to run the main coroutine\n",
    "all_ten_k_document_lists_of_dicts=asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebac143d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34d178d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is final\n",
    "async def extracting_stock_compensation_for_one_company(one_title_searchable):\n",
    "\n",
    "    async def getting_10_k_link(one_title_searchable):\n",
    "        #### ON THE SEC HOME PAGE\n",
    "        # going to sec page\n",
    "        sec_home_playwright = await async_playwright().start()\n",
    "        sec_home_browser = await sec_home_playwright.chromium.launch(headless = False)\n",
    "        sec_home_page = await sec_home_browser.new_page()\n",
    "\n",
    "        await sec_home_page.goto(\"https://www.sec.gov/edgar/searchedgar/companysearch\")\n",
    "        time.sleep(6)\n",
    "\n",
    "        try:\n",
    "\n",
    "            # input company name (here is where we can enter title variable from df from csv)\n",
    "\n",
    "            await sec_home_page.fill(\"xpath=/html/body/div[1]/div/div/div/div[3]/div/div[2]/div[2]/div/div/article/div/div/div[1]/div[2]/div/div/div/div[1]/form/div[2]/input\", one_title_searchable)\n",
    "            time.sleep(6)\n",
    "\n",
    "            # click search\n",
    "\n",
    "            await sec_home_page.click(\"xpath=/html/body/div[1]/div/div/div/div[3]/div/div[2]/div[2]/div/div/article/div/div/div[1]/div[2]/div/div/div/div[1]/form/div[2]/button\")\n",
    "            time.sleep(6)\n",
    "\n",
    "            #============\n",
    "            # ON THE NEW PAGE RESULTING WITH ALL COMPANY DOCS\n",
    "            #input 10K for search\n",
    "            await sec_home_page.fill(\"xpath=/html/body/div[4]/div[2]/form/table/tbody/tr[3]/td[1]/input\", \"10-K\")\n",
    "            time.sleep(6)\n",
    "\n",
    "            # click search\n",
    "\n",
    "            await sec_home_page.click(\"xpath=/html/body/div[4]/div[2]/form/table/tbody/tr[3]/td[5]/input[1]\")\n",
    "            time.sleep(6)\n",
    "\n",
    "\n",
    "            #get all 10K entries\n",
    "            list_of_all_ten_k_entries_response= await sec_home_page.content()\n",
    "\n",
    "            list_of_all_ten_k_entries_doc = BeautifulSoup(list_of_all_ten_k_entries_response, 'html.parser')\n",
    "\n",
    "            #get the first 10K\n",
    "            list_of_all_ten_k_entries_table = list_of_all_ten_k_entries_doc.find('table', class_='tableFile2')\n",
    "\n",
    "            ten_k_entries_rows = list_of_all_ten_k_entries_table.find_all('tr')  # get the first row because it has the most recent 10K\n",
    "\n",
    "            most_recent_ten_k_link = None\n",
    "\n",
    "            for one_ten_k_entries_row in ten_k_entries_rows:\n",
    "                one_ten_k_entries_row_cells = one_ten_k_entries_row.find_all('td')  # Assuming each cell is represented by a <td> tag\n",
    "            #     print(one_ten_k_entries_row_cells)\n",
    "\n",
    "                for one_ten_k_entries_row_cell in one_ten_k_entries_row_cells:\n",
    "            #         print(one_ten_k_entries_row_cell)\n",
    "                    if one_ten_k_entries_row_cell.text.strip() == '10-K':\n",
    "                        ten_k_document_link_href = one_ten_k_entries_row_cell.find_next('td').find('a')['href']\n",
    "\n",
    "                        ten_k_document_link_base = \"https://www.sec.gov/\"\n",
    "                        ten_k_document_link_tail = ten_k_document_link_href\n",
    "                        ten_k_document_link = ten_k_document_link_base + ten_k_document_link_tail\n",
    "\n",
    "            #             print(document_link)\n",
    "\n",
    "                        # Store the first 10-K link and break out of the loop\n",
    "                        most_recent_ten_k_link = ten_k_document_link\n",
    "                        break\n",
    "\n",
    "                if most_recent_ten_k_link is not None:\n",
    "                    break\n",
    "\n",
    "\n",
    "            # ONCE I HAVE THE TEN K DOC LINK \n",
    "            # 3. Now on the page with the document and graphics table\n",
    "\n",
    "            ten_k_doc_link_playwright = await async_playwright().start()\n",
    "            ten_k_doc_link_browser = await ten_k_doc_link_playwright.chromium.launch(headless = False)\n",
    "            ten_k_doc_link_page = await ten_k_doc_link_browser.new_page()\n",
    "\n",
    "            time.sleep(6)\n",
    "\n",
    "            await ten_k_doc_link_page.goto(most_recent_ten_k_link)\n",
    "            time.sleep(6)\n",
    "\n",
    "\n",
    "\n",
    "            # Initiate the html and beautiful soup content \n",
    "\n",
    "            list_of_doc_and_graphics_response= await ten_k_doc_link_page.content()\n",
    "\n",
    "            list_of_doc_and_graphics_soup_doc = BeautifulSoup(list_of_doc_and_graphics_response, 'html.parser')\n",
    "\n",
    "            # get actual_full_ten_k_document_link\n",
    "            list_of_doc_and_graphics_table = list_of_doc_and_graphics_soup_doc.find('table', class_='tableFile')\n",
    "\n",
    "\n",
    "            list_of_doc_and_graphics_rows = list_of_doc_and_graphics_table.find_all('tr')  # Assuming each row is represented by a <tr> tag\n",
    "\n",
    "            for list_of_doc_and_graphics_row in list_of_doc_and_graphics_rows:\n",
    "                list_of_doc_and_graphics_cells = list_of_doc_and_graphics_row.find_all('td')  # Assuming each cell is represented by a <td> tag\n",
    "            #     print(cells)\n",
    "                for list_of_doc_and_graphics_cell in list_of_doc_and_graphics_cells:\n",
    "                    if list_of_doc_and_graphics_cell.text.strip() == '10-K':\n",
    "                        actual_document_link = list_of_doc_and_graphics_cell.find_next('td').find('a')\n",
    "                        if actual_document_link is not None:\n",
    "                            actual_ten_k_document_href = actual_document_link['href']\n",
    "            #                 print(\"Documents href:\", document_href)\n",
    "                            actual_ten_k_document_link_base=\"https://www.sec.gov/\"\n",
    "                            actual_ten_k_document_link_tail=actual_ten_k_document_href\n",
    "                            actual_full_ten_k_document_link=actual_ten_k_document_link_base+actual_ten_k_document_link_tail\n",
    "                            actual_full_ten_k_document_link=x\n",
    "                            print(actual_full_ten_k_document_link)\n",
    "                            print(\"#########\")\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        finally:\n",
    "            # Close the page and browser\n",
    "            await ten_k_doc_link_page.close()\n",
    "            await ten_k_doc_link_browser.close()\n",
    "            await ten_k_doc_link_playwright.stop()\n",
    "\n",
    "            # Close the sec_home page and browser\n",
    "            await sec_home_page.close()\n",
    "            await sec_home_browser.close()\n",
    "            await sec_home_playwright.stop()\n",
    "\n",
    "    # with change for link\n",
    "\n",
    "\n",
    "    list_of_actual_full_ten_k_document_links = [\"https://www.sec.gov/ix?doc=/Archives/edgar/data/789019/000156459022026876/msft-10k_20220630.htm\"]\n",
    "\n",
    "    actual_list_of_actual_full_ten_k_document_links = [\"https://www.sec.gov//ix?doc=/Archives/edgar/data/320193/000032019322000108/aapl-20220924.htm\",\"https://www.sec.gov/ix?doc=/Archives/edgar/data/789019/000156459022026876/msft-10k_20220630.htm\",\"https://www.sec.gov/ix?doc=/Archives/edgar/data/1652044/000165204423000016/goog-20221231.htm\"]\n",
    "\n",
    "    async def get_actual_full_ten_k_document_content(x):\n",
    "        playwright = await async_playwright().start()\n",
    "        browser = await playwright.chromium.launch(headless=False)\n",
    "    #     browser = await playwright.firefox.launch(headless=False)\n",
    "\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        await page.goto(x)\n",
    "        time.sleep(10)\n",
    "\n",
    "        while True:\n",
    "            # Get the number of spans on the page\n",
    "            spans_assigned = await page.query_selector_all(\"span\")\n",
    "            test = len(spans_assigned)\n",
    "\n",
    "            # Scroll in increments of 5000 pixels\n",
    "            await page.evaluate('''() => {\n",
    "                let distance = 5000;\n",
    "                window.scrollBy(0, distance);\n",
    "            }''')\n",
    "    #         await asyncio.sleep(3)\n",
    "\n",
    "            time.sleep(10)\n",
    "\n",
    "\n",
    "            spans_assigned_two = await page.query_selector_all(\"span\")\n",
    "            test2 = len(spans_assigned_two)\n",
    "            if test == test2:\n",
    "                break\n",
    "\n",
    "        # Get the content of the page\n",
    "        actual_full_ten_k_document_content = await page.content()\n",
    "        time.sleep(10)\n",
    "\n",
    "        await browser.close()\n",
    "        time.sleep(10)\n",
    "        await page.close()  # Add this line to close the page\n",
    "        time.sleep(10)\n",
    "\n",
    "        return actual_full_ten_k_document_content\n",
    "\n",
    "\n",
    "    async def process_actual_full_ten_k_document(actual_full_ten_k_document_content):\n",
    "        actual_full_ten_k_document_soup = BeautifulSoup(actual_full_ten_k_document_content, 'html.parser')\n",
    "        return actual_full_ten_k_document_soup\n",
    "\n",
    "\n",
    "    async def main():\n",
    "        dict_titles=['actual_full_ten_k_document_link_collection',\n",
    "                     'table_length',\n",
    "                     'list_of_stock_based_values',\n",
    "                     'first_stock_based_compensation',\n",
    "                     'second_stock_based_compensation',\n",
    "                     'third_stock_based_compensation']\n",
    "\n",
    "        possible_titles = ['CONSOLIDATED STATEMENTS OF CASH FLOWS',\n",
    "                           'CASH FLOWS STATEMENTS',\n",
    "                           'Consolidated Statements of Cash Flows',\n",
    "                           'Consolidated Statement of Cash Flows',\n",
    "                          'CONSOLIDATED STATEMENT OF CASH FLOWS',\n",
    "                          'STATEMENTS OF CONSOLIDATED CASH FLOWS',\n",
    "                          'Consolidated Cash Flow Statement',\n",
    "                          'Consolidated Statement of Cash Flow for the Years Ended December 31',\n",
    "                          'STATEMENT OF CASH FLOWS',\n",
    "                          'CONSOLIDATED STATEMENTS OF CASH FLOW',\n",
    "                          'Statements of Consolidated Cash Flows']\n",
    "\n",
    "        stock_related_texts = [\n",
    "            'Stock-based compensation expense',\n",
    "            'Stock-based compensation expense',\n",
    "            'Share-based compensation expense',\n",
    "            'Stock-based compensation',\n",
    "            'Share-based compensation'\n",
    "        ]\n",
    "\n",
    "        all_ten_k_document_lists = []\n",
    "\n",
    "\n",
    "    #     list_of_example_actual_full_ten_k_document_soup_doc = []\n",
    "\n",
    "        # Define the group size\n",
    "        group_size = 2\n",
    "        items=df_stock_top_us_plus_sec_titles['actual_full_ten_k_document_link']\n",
    "\n",
    "        # Iterate over groups of items\n",
    "        for start_index in range(0, len(items), group_size):\n",
    "            group_items = items[start_index:start_index + group_size]\n",
    "\n",
    "            list_of_example_actual_full_ten_k_document_soup_doc = []\n",
    "\n",
    "\n",
    "    #         # Iterate over items within the group\n",
    "    #         for item in group_items:\n",
    "    #             print(item)\n",
    "\n",
    "    #         # Add a separator for each group\n",
    "    #         print(\"Group separator\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            url_to_parsed_content = {}\n",
    "\n",
    "\n",
    "            for actual_full_ten_k_document_link in group_items:\n",
    "                try:\n",
    "                    actual_full_ten_k_document_link_collection = actual_full_ten_k_document_link\n",
    "                    print(actual_full_ten_k_document_link)\n",
    "\n",
    "                    for _ in range(3):\n",
    "                        try:\n",
    "                            actual_full_ten_k_document_content = await get_actual_full_ten_k_document_content(actual_full_ten_k_document_link)\n",
    "                            example_actual_full_ten_k_document_soup_doc = await process_actual_full_ten_k_document(actual_full_ten_k_document_content)\n",
    "                            list_of_example_actual_full_ten_k_document_soup_doc.append(example_actual_full_ten_k_document_soup_doc)\n",
    "                                # Store the parsed content in the dictionary.\n",
    "                            url_to_parsed_content[actual_full_ten_k_document_link] = example_actual_full_ten_k_document_soup_doc\n",
    "\n",
    "                            break  # Break the loop if successful\n",
    "                        except Exception as e:\n",
    "                            print(f\"An error occurred: {str(e)}. Retrying...\")\n",
    "                            time.sleep(15)\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "            for actual_full_ten_k_document_link, one_actual_full_ten_k_document_soup_doc in url_to_parsed_content.items():\n",
    "                ten_k_detail_list=[]\n",
    "\n",
    "                if one_actual_full_ten_k_document_soup_doc:\n",
    "\n",
    "                    elements = one_actual_full_ten_k_document_soup_doc.find_all(text=possible_titles)\n",
    "\n",
    "                    if not elements:\n",
    "                        print(\"Titles not found in the HTML.\")\n",
    "                        continue\n",
    "\n",
    "                    for element in elements:\n",
    "                        if element.strip() in possible_titles:\n",
    "                            following_elements = element.find_all_next()\n",
    "                            table = None\n",
    "\n",
    "                            # Iterate through following elements and stop when a table is found or after three components\n",
    "                            component_count = 0\n",
    "                            for next_element in following_elements:\n",
    "                                if next_element.name == 'table' and any(text in next_element.get_text() for text in stock_related_texts):\n",
    "                                    table = next_element\n",
    "                                    break\n",
    "                                elif next_element.name in ['p', 'span']:\n",
    "                                    component_count += 1\n",
    "                                    if component_count >= 5:\n",
    "                                        break\n",
    "\n",
    "                            if table is None:\n",
    "                                tables = one_actual_full_ten_k_document_soup_doc.find_all(\"table\")\n",
    "                                for t in tables:     \n",
    "                                    if t:\n",
    "                                        table_text = t.get_text()\n",
    "                                        if any(title in table_text for title in possible_titles) and any(stock_related_text in table_text for stock_related_text in stock_related_texts):\n",
    "                                            rows = t.find_all('tr')\n",
    "                                            table_length = len(rows)\n",
    "                                            print(f\"Table Length from within: {len(rows)}\") \n",
    "                                        break\n",
    "\n",
    "                            if table:\n",
    "                                rows = table.find_all('tr')\n",
    "                                table_length=len(rows)\n",
    "\n",
    "                                print(f\"Table Length: {len(rows)}\")                        \n",
    "\n",
    "                                break\n",
    "\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        print(\"No table found within or after the possible titles in the HTML.\")\n",
    "\n",
    "                else:\n",
    "                    print(\"Document not processed, skipping\")\n",
    "\n",
    "\n",
    "                if table:\n",
    "                    target_td = table.find('td', text=stock_related_texts)\n",
    "                else:\n",
    "                    target_td = None\n",
    "\n",
    "                if target_td:\n",
    "                    # Move up to the <tr> containing the <td> with \"share-based compensation\"\n",
    "                    target_tr = target_td.find_parent('tr')\n",
    "\n",
    "                    # Extract all the <td> values from the <tr> and put them in a column\n",
    "                    values_in_column = [td.get_text(strip=True) for td in target_tr.find_all('td')]\n",
    "\n",
    "                    # Output the values in the column\n",
    "                    if values_in_column:\n",
    "                        stock_stock_based_compensation_title=values_in_column[0]\n",
    "                        list_of_stock_based_values=values_in_column\n",
    "                        numbers_only_values_in_column = [entry for entry in values_in_column if any(char.isdigit() for char in entry)]\n",
    "                        first_stock_based_compensation=numbers_only_values_in_column[0]\n",
    "                        second_stock_based_compensation=numbers_only_values_in_column[1]\n",
    "                        third_stock_based_compensation=numbers_only_values_in_column[2]\n",
    "                        for value in values_in_column:\n",
    "                            print(value)\n",
    "\n",
    "                    ten_k_detail_list.append(actual_full_ten_k_document_link)\n",
    "                    ten_k_detail_list.append(table_length)\n",
    "                    ten_k_detail_list.append(list_of_stock_based_values)\n",
    "                    ten_k_detail_list.append(first_stock_based_compensation)\n",
    "                    ten_k_detail_list.append(second_stock_based_compensation)\n",
    "                    ten_k_detail_list.append(third_stock_based_compensation)\n",
    "\n",
    "                    first_rep = dict(zip(dict_titles, ten_k_detail_list))\n",
    "                    all_ten_k_document_lists.append(first_rep)\n",
    "\n",
    "\n",
    "\n",
    "                else:\n",
    "                    print(\"The 'Share-Based Compensation' row not found!\")\n",
    "                time.sleep(10)\n",
    "\n",
    "    #     print(all_ten_k_document_lists)\n",
    "        return all_ten_k_document_lists\n",
    "\n",
    "\n",
    "    # Run the main coroutine\n",
    "    nest_asyncio.apply()\n",
    "\n",
    "    # Use asyncio.run() to run the main coroutine\n",
    "    all_ten_k_document_lists_of_dicts=asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb02490",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5605a802",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf4277a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7d407a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115829ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702d711c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcf5b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223a5e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "await extracting_stock_compensation_for_one_company(one_title_searchable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47a5eaa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802470bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca77048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582b9f24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40608e64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1171df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e4451c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84e285e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
